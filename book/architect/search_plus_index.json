{"./":{"url":"./","title":"前言","keywords":"","body":"简介参考简介 架构师基础是由我在极客时间从零开始学架构专栏和>书籍学习后整理而来 参考 从零开始学架构 > Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/basic/isarchitect.html":{"url":"docs/basic/isarchitect.html","title":"架构是什么","keywords":"","body":"架构是什么架构是什么 架构是顶层设计 几个基础的概念 模块-组件 模块逻辑职责划分 组件是物理单元复用划分 系统 多个有关联的个体按照一定规则下产生新的能力，能够独立运行的实体 架构-框架 架构是顶层设计 框架是面向编程或配置的半成品。 框架是组件规范，提供基础功能的产品。 架构关注的是“结构”，框架关注的是“规范” Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/basic/history.html":{"url":"docs/basic/history.html","title":"架构的历史背景","keywords":"","body":"架构的历史背景架构的历史背景 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/basic/purpose.html":{"url":"docs/basic/purpose.html","title":"架构设计目的","keywords":"","body":"架构设计的目的高性能任务分配任务分解高可用计算高可用存储高可用高可用状态决策可扩展性预测变化应对变化低成本、安全、规模低成本安全规模架构设计的目的 架构设计的主要目的是为了解决软件系统复杂度带来的问题 高性能 任务分配 每台机器都可以处理完整的业务任务，不同的任务分配到不同的机器上执行 任务分解 业务服务器如果越来越复杂，可以将其拆分为更多的组成部分。通过这种方式，能够把原来大一统但复杂的业务系统，拆分成小而简但需要多个系统配合的业务系。 任务分解的优点 简单的系统更加容易做到高性能 系统越简单，影响性能的点就越少，更加容易进行有针对性的优化。 系统复杂度情况下，首先难以找到关键性能点（需要验证和考虑的点太多），其次是即使找到了，修改起来也不太容易（可能A关键性能点提升，但却无意将B点的性能降低了） 可以针对单个任务进行扩展 发现系统瓶颈后只需要对有瓶颈的子系统进行性能优化或者提升，不需要改动整个系统，风险会小很多。 通过任务分配到方式可以突破单台机器处理性能的瓶颈，通过增加更多的机器来解决性能的需求，当业务越来越复杂，单台机器处理的性能会越来越低，为了能够继续提升性能，需要采取任务分解方式。分解的子系统也不能太多，否则太对子系统之间的调用会降低性能。 高可用 做到冗余备份和失效转移。 当任何一台服务器宕机或出现各种不可预期的问题时，就将相应的服务切换到其他可用的服务器上，不影响系统的整体可用性，也不会导致数据的丢失。 计算高可用 无论在哪台机器上进行计算，同样的算法和输入数据，产出的结果都是一样的。 存储高可用 存储需要将数据从一台机器搬到另一台机器，需要经过线路进行传输，这个过程是比较耗时的，可能会产生数据不一致。存储高可用在于减少或者规避数据不一致对业务造成的影响。CAP定理(Consistency, Availability, Partition ToLerance) 一致性 可用性 分区容错性 高可用状态决策 独裁式 存在一个独立的决策主体（决策者），负责收集信息然后进行决策；所有冗余的个体（上报者），都将状态信息发送给决策者。 问题 决策者本身故障，整个系统就无法实现准确的状态决策。 协商式 两个独立的个体通过交流信息，然后根据规则进行决策 问题 主机备机连接中断，备机需要升级为主机，但实际上主机并没有故障 连接中断的情况下备机不认为主机故障，而主机真的发生故障。 建立更多的连接。能够降低连接中断对状态的影响，但如果不同链接传递的信息不同，不确定哪一个为准。 民主式 多个独立的个体通过投票的方式来进行状态决策。 问题 脑裂。原来统一的集群因为连接中断，造成了两个独立分隔的子集群，每个子集群单独进行选举，于是出现了两个主机。 通过投票数量超过一半的规则来解决脑裂问题，但是降低了系统整体的可用性，如果系统不是因为脑裂问题导致投票节点数过少，而是因为节点故障，此时系统也不会选出主节点，导致整个系统宕机（尽管某些节点还是正常的）。 无论采用什么方案，状态决策都不可能做到任何场景下都没问题。 如何选取适合系统的高可用方案，需要复杂度分析、判断和选择。 可扩展性 正确预测变化、完美封装变化。 预测变化 不能每个设计点都考虑可扩展性 不能完全不考虑可扩展性 所有的预测都存在出错的可能性 应对变化 将“变化”封装在一个“变化层”，将不变的部分封装在一个独立的“稳定层” 系统需要拆分出变化层和稳定层 变化层和稳定层需要人为的判断理解 需要设计变化层和稳定层之间的接口 接口越稳定越好。 对于变化层来说，在有差异的多个实现方式中找到共同点，并且还要保证当加入新的功能时原有的接口设计不需要太大的修改，是一件很复杂的事情。 提炼出一个“抽象层”和一个“实现层”。抽象层是稳定的，实现层可以根据具体业务需要定制开发，当加入新的功能时，只需要增加新的实现，无需修改抽象层（设计模式和规则引擎）。 设计模式的核心：封装变化，隔离可变性。 低成本、安全、规模 低成本 低成本是架构设计中需要考虑的一个约束条件，但不是首要目标。一般通过“创新”达到低成本目标 引入新技术 开创全新的技术领域 安全 安全是一个庞大而又复杂度技术领域。 功能安全 与编码实现有关，问题出现后有针对性的提出解决方案。 架构安全 防火墙：功能强大，性能一般，成本高 自己设计与实现 规模 规模带来的复杂度主要是“量变引起质变”，当数据超过一定的阈值后，复杂度会发生质的变化。 逻辑复杂度 数据越来越多 规模问题需要与高性能、高可用、高扩展、高伸缩性统一考虑。常采用“分而治之，逐个击破”的方法策略。如mysql单表数据量太多，进行拆表（按照什么规则拆表，拆表后查询如何处理等） Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/basic/principles.html":{"url":"docs/basic/principles.html","title":"架构设计原则","keywords":"","body":"架构设计三原则合适原则简单原则演化原则架构设计三原则 架构即决策。架构需要面向业务需求，并在各种资源（人、财、物、时、事）约束条件下去做权衡、取舍。决策会存在不确定性。采用一些高屋建瓴的设计原则有助于消除不确定，去逼近解决问题的最优解。 合适原则 合适优于业界领先真正优秀的架构都是在企业当前人力、条件、业务等各种约束下设计出来的，能够合理的将资源整合在一起并发挥出最大功效，并且能够快速落地。 简单原则 简单优于复杂 复杂性体现 结构的复杂性 组成复杂系统的组件数量更多 同时这些组件之间的关系也更加复杂 组件越多，就越有可能其中某个组件出现故障 某个组件改动，会影响关联的所有组件 定位一个复杂系统中断问题总是比简单系统更加困难 逻辑的复杂性 逻辑复杂度组件，一个典型的特征就是单个组件承担了太多的功能。 无论是结构的复杂性，还是逻辑的复杂性，都会存在各种问题，所以架构设计时如果简单的方案和复杂的方案都可以满足需求，最好选择简单的方案。 演化原则 演化优于一步到位软件需要根据业务等发展不短的变化 软件架构设计过程 设计出来的架构要满足当时的业务需要 架构要不断的在实际应用过程中迭代，保留优秀的设计，修复有缺陷的设计，改正错误的设计，去掉无用的设计，使得架构逐渐完善 当业务发生变化时，架构要扩展、重构、甚至重写；代码也许会重写，但有价值的经验、教训、逻辑、设计等却可以在新的架构中延续 时刻提醒自己不要贪大求全，或者盲目照搬大公司的做法。应该认真分析当前业务的特点，明确业务面临的主要问题，设计合理的架构，快速落地以满足业务需要，然后在运行过程中不断完善架构，不断随着业务演化架构。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/basic/process.html":{"url":"docs/basic/process.html","title":"架构设计流程","keywords":"","body":"架构设计流程识别复杂度设计备选方案设计方案常见误区备选方案评估和选择备选详细方案设计架构设计流程 常见性能量级具体的数值与机器配置和测试案例有关。nginx负载均衡性能3w左右，mc的读取性能5w左右，kafka号称百万级，zookeeper写入读取2w以上，http请求放问2w左右。 识别复杂度 将主要的复杂度问题列出来，然后根据业务、技术、团队等综合情况进行排序，优先解决当前面临的最主要的复杂度问题。识别复杂度为了明确设计方案的目标 设计备选方案 设计方案常见误区 设计最优秀的方案 设计适合自己业务、团队、技术能力的方案才是好方案 只做一个方案 心里评估过于简单，可能没有想得全面 经验知识和技能有局限，有可能某个评估的标准或者经验不正确。 单一的方案设计会出现过度辩护的情况 备选方案过于详细 耗费大量的精力和时间 将注意力集中到细节中，容易忽略整体的技术设计，导致备选方案数量不够或者差异不明显 评审的时候其他人会被很多细节给绕进去，评审效果差。 备选方案 备选方案都数量已3~5个为最佳 太少可能考虑不周，过多又需要耗费大量的精力和时间。 备选方案的差异要比较明显 备选方案的技术不要只局限于已经熟悉的技术 架构师需要将视野放宽，考虑更多的可能性。 备选阶段关注的是技术选型，而不是技术细节，技术选型的差异要比较明显。 评估和选择备选 360度环评: 列出我们需要关注的质量属性点，然后分别从这些质量属性的维度去评估每个方案，再综合挑选适合当时情况的最优方案 常见的方案质量属性点: 性能、可用性、硬件成本、项目投入、复杂度、安全性、可扩展性等。 按照优先级选择备选方案即架构师综合当前的业务发展情况、团队人员规模和技能、业务发展预测等因素，将质量属性按照优先级排序，首先挑选满足第一优先级的，如果方案都满足，那就再看第二优先级……以此类推。 详细方案设计 详细设计方案阶段可能遇到的一种极端情况就是在详细设计阶段发现备选方案不可行，一般情况下主要的原因是备选方案设计时遗漏了某个关键技术点或者关键的质量属性。 避免在详细阶段发现备选方案不可行 架构师不但要进行备选方案设计和选型，还需要对备选方案的关键细节有较深入的理解。 通过分步骤、分阶段、分系统等方式，尽量降低方案复杂度 如果方案本身就很复杂，那就采取设计团队的方式来进行设计，博采众长，汇集大家的智慧和经验。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/hp/store.html":{"url":"docs/hp/store.html","title":"存储高性能","keywords":"","body":"存储高性能高性能数据库集群读写分离分库分表分库分表实现方法高性能NoSQL关系数据库缺点关系数据库优点常见NoSQL方案分为4类高性能缓存架构缓存穿透缓存雪崩存储高性能 高性能数据库集群 读写分离 基本原理是将数据库读写操作分散到不同的节点上 读写分离的基本实现 数据库服务器搭建主从集群 数据库主机负责读写操作，从机只负责读操作 数据库主机通过复制将数据同步到从机，每台数据库服务器都存储了所有的业务数据 业务服务器将写操作发给数据库主机，将读操作发给数据库从机。 读写分离引入的复杂度 主从复制延迟 当访问的是从机时，而主机还没有将最新的更新数据更新到从机，会导致读取不到数据。 解决 写操作后的读操作指定发给数据库主服务器 和业务强绑定, 对业务的侵入和影响较大 读从机失败后再读一次主机 如果有很多二次读取,主机可能顶不住读操作压力而崩溃 关键业务读写操作全部指向主机, 非关键业务采用读写分离 分配机制 程序代码封装 程序代码封装指在代码中抽象一个数据访问层，实现读写操作分离和数据库服务器连接的管理 实现简单，而且可以根据业务做较多定制化的功能 每个编程语言都需要自己实现一次，无法通用 故障情况下，如果主从发生切换，则可能需要所有系统都修改配置并重启 中间件封装 中间件封装指的是独立一套系统出来，实现读写操作分离和数据库服务器连接的管理。中间件对业务服务器提供 SQL 兼容的协议，业务服务器无须自己进行读写分离。对于业务服务器来说，访问中间件和访问数据库没有区别，事实上在业务服务器看来，中间件就是一个数据库服务器 能够支持多种编程语言，因为数据库中间件对业务服务器提供的是标准 SQL 接口 数据库中间件要支持完整的 SQL 语法和数据库服务器的协议，实现比较复杂，细节特别多，很容易出现 bug，需要较长的时间才能稳定。 数据库中间件自己不执行真正的读写操作，但所有的数据库操作请求都要经过中间件，中间件的性能要求也很高。 数据库主从切换对业务服务器无感知，数据库中间件可以探测数据库服务器的主从状态。 分库分表 分库 业务分库指的是按照业务模块将数据分散到不同的数据库服务器。 分库带来的问题 join操作问题 事务问题 需采用程序模拟操作实现事务. 成本问题 业务分库需要更多的服务器,需要更大的成本 分表 单表拆分两种方式 垂直分表 垂直分表适合将表中某些不常用且占了大量空间的列拆分出去。 垂直分表引入的复杂性主要体现在表操作的数量要增加 水平分表 水平分表适合表行数特别大的表 水平分表带来的问题 路由水平分表后，某条数据具体属于哪个切分后的子表，需要增加路由算法进行计算，这个算法会引入一定的复杂性 范围路由 选取有序的数据列作为路由的条件，不同分段分散到不同的数据库表中。 范围路由设计的复杂点主要体现在分段大小的选取上 范围路由的一个比较隐含的缺点是分布不均匀 hash路由 选取某个列（或者某几个列组合也可以）的值进行 Hash 运算，然后根据 Hash 结果分散到不同的数据库表中。 Hash 路由设计的复杂点主要体现在初始表数量的选取上 用了 Hash 路由后，增加字表数量是非常麻烦的，所有数据都要重分布 配置路由 配置路由就是路由表，用一张独立的表来记录路由信息。 配置路由的缺点就是必须多查询一次，会影响整体性能，如果路由表本身太大，性能同样成为瓶颈。 join操作水平分表后，数据分散在多个表中，如果需要与其他表进行 join 查询，需要在业务代码或者数据库中间件中进行多次 join 查询，然后将结果合并 count()操作 count()相加 具体做法是在业务代码或者数据库中间件中对每个表进行 count() 操作，然后将结果相加 实现简单，但性能较低。 记录数表 具体做法是新建一张表，用来记录count的结果，每次有数据增加和删除，修改相应的count的值 性能很高，但复杂度高，如果有遗漏就会导致数据不一致；而且增加了数据库的写压力。 对于不要求记录数实时的业务，可以结合两种方法，使用count()相加的方式定时更新记录数表 order by操作水平分表后，数据分散到多个子表中，排序操作无法在数据库中完成，只能由业务代码或者数据库中间件分别查询每个子表中的数据，然后汇总进行排序。 分库分表实现方法 和数据库读写分离类似，分库分表具体的实现方式也是“程序代码封装”和“中间件封装”，但实现会更复杂。 高性能NoSQL 关系数据库缺点 关系数据库存储的是行记录，无法存储数据结构 关系数据库的 schema 扩展很不方便 关系数据库在大数据场景下 I/O 较高 关系数据库的全文搜索功能比较弱 关系数据库优点 关系数据库按照行式来存储数据 业务同时读取多个列时效率高 能够一次性完成对一行中的多个列的写操作，保证了针对行数据写操作的原子性和一致性 常见NoSQL方案分为4类 K-V 存储 解决关系数据库无法存储数据结构的问题，以 Redis 为代表。 Redis的主要缺点是不支持完整的 ACID 事务, Redis 的事务只能保证隔离性和一致性（I 和 C），无法保证原子性和持久性（A 和 D）。 文档数据库 解决关系数据库强 schema 约束的问题，以 MongoDB 为代表。 文档数据库最大的特点就是 no-schema，可以存储和读取任意的数据大部分文档数据库存储的数据格式是 JSON（或者 BSON） 优势 新增字段简单 历史数据不会出错 很容易存储复杂数据 列式数据库 解决关系数据库大数据场景下的 I/O 问题，以 HBase 为代表。 列式存储将不同列存储在磁盘上不连续的空间 相比于行式存储有更高的压缩率 使用场景: 一般将列式存储应用在离线的大数据分析和统计场景中，因为这种场景主要是针对部分列单列进行操作，且数据写入后就无须再更新删除 全文搜索引擎 解决关系数据库的全文搜索性能问题，以 Elasticsearch 为代表。 全文搜索中关系数据库表现 全文搜索的条件可以随意排列组合，如果通过索引来满足，则索引的数量会非常多 全文搜索的模糊匹配方式，索引无法满足，只能用 like 查询，而 like 查询是整表扫描，效率非常低。 全文搜索基本原理全文搜索引擎的技术原理被称为“倒排索引”（Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，其基本原理是建立单词到文档的索引 全文搜索的使用方式全文搜索引擎的索引对象是单词和文档，而关系数据库的索引对象是键和行，两者的术语差异很大，不能简单地等同起来。因此，为了让全文搜索引擎支持关系型数据的全文搜索，需要做一些转换操作，即将关系型数据转换为文档数据。目前常用的转换方式是将关系型数据按照对象的形式转换为 JSON 文档，然后将 JSON 文档输入全文搜索引擎进行索引 高性能缓存架构 缓存就是为了弥补存储系统在这些复杂业务场景下的不足，其基本原理是将可能重复使用的数据放到内存中，一次生成、多次使用，避免每次使用都去访问存储系统。 缓存穿透 缓存穿透是指缓存没有发挥作用，业务系统虽然去缓存查询数据，但缓存中没有数据，业务系统需要再次去存储系统查询数据。 存储数据不存在 缓存数据生成耗费大量时间或者资源 存储系统中存在数据，但生成缓存数据需要耗费较长时间或者耗费大量资源. 如果刚好在业务访问的时候缓存失效了，那么也会出现缓存没有发挥作用，访问压力全部集中在存储系统上的情况。 缓存雪崩 缓存雪崩是指当缓存失效（过期）后引起系统性能急剧下降的情况。 当缓存过期被清除后，业务系统需要重新生成缓存，因此需要再次访问存储系统，再次进行运算，这个处理步骤耗时几十毫秒甚至上百毫秒。而对于一个高并发的业务系统来说，几百毫秒内可能会接到几百上千个请求。由于旧的缓存已经被清除，新的缓存还未生成，并且处理这些请求的线程都不知道另外有一个线程正在生成缓存，因此所有的请求都会去重新生成缓存，都会去访问存储系统，从而对存储系统造成巨大的性能压力。这些压力又会拖慢整个系统，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃。 解决方案 更新锁机制 对缓存更新操作进行加锁保护，保证只有一个线程能够进行缓存更新 后台更新机制 由后台线程来更新缓存，而不是由业务线程来更新缓存，缓存本身的有效期设置为永久，后台线程定时更新缓存。 特殊情况当缓存系统内存不够时，会“踢掉”一些缓存数据，从缓存被“踢掉”到下一次定时更新缓存的这段时间内，业务线程读取缓存返回空值，而业务线程本身又不会去更新缓存，因此业务上看到的现象就是数据丢了。 解决 后台线程除了定时更新缓存，还要频繁地去读取缓存, 如果发现缓存被“踢了”就立刻更新缓存(读取间隔太长的话, 在这个间隔时间内业务访问也拿不到真正的数据而返回一个空值) 业务线程发现缓存失效后，通过消息队列发送一条消息通知后台线程更新缓存 缓存热点 缓存热点的解决方案就是复制多份缓存副本，将请求分散到多个缓存服务器上，减轻缓存热点导致的单台缓存服务器压力。 缓存副本设计有一个细节需要注意，就是不同的缓存副本不要设置统一的过期时间，否则就会出现所有缓存副本同时生成同时失效的情况，从而引发缓存雪崩效应。 正确的做法是设定一个过期时间范围，不同的缓存副本的过期时间是指定范围内的随机值。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/hp/calcul.html":{"url":"docs/hp/calcul.html","title":"计算高性能","keywords":"","body":"计算高性能单服务器高性能模式PPC与TPCReactor和Proactor高性能负载均衡分类与架构算法计算高性能 单服务器高性能模式 架构设计决定了系统性能的上限，实现细节决定了系统性能的下限。 网络编程模型设计关键点 服务器如何管理连接 服务器如何处理请求 设计的两个关键点和操作系统的i/o模型以及进程模型相关 i/o模型：阻塞、非阻塞、同步、异步 进程模型：单进程、多进程、多线程 PPC与TPC PPC PPC 是 Process Per Connection 的缩写，其含义是指每次有新的连接就新建一个进程去专门处理这个连接的请求，这是传统的 UNIX 网络服务器所采用的模型。 父进程接受连接。 父进程“fork”子进程。 子进程处理连接的读写请求。 子进程关闭连接 PPC 模式实现简单，比较适合服务器的连接数没那么多的情况 缺点 fork代价高 父子进程通信复杂 支持的并发连接数量有限 改进preforkPPC 模式中，当连接进来时才 fork 新进程来处理连接请求，由于 fork 进程代价高，用户访问时可能感觉比较慢prefork 就是提前创建进程（pre-fork） TPC TPC 是 Thread Per Connection 的缩写，其含义是指每次有新的连接就新建一个线程去专门处理这个连接的请求。 父进程接受连接 父进程创建子线程 子线程处理连接的读写请求 子线程关闭连接 缺点 创建线程虽然比创建进程代价低，但并不是没有代价，高并发时（例如每秒上万连接）还是有性能问题。 无须进程间通信，但是线程间的互斥和共享又引入了复杂度，可能一不小心就导致了死锁问题。 多线程会出现互相影响的情况，某个线程出现异常时，可能导致整个进程退出（例如内存越界）。 TPC 方案本质上和 PPC 方案基本类似 改进prethread 和 prefork 类似，prethread 模式会预先创建线程 总结PPC和TPC优点是实现简单, 缺点是都无法支撑高并发场景. Reactor和Proactor I/O多路复用归纳 当多条连接共用一个阻塞对象后，进程只需要在一个阻塞对象上等待，而无须再轮询所有连接，常见的实现方式有 select、epoll、kqueue 等。 当某条连接有新的数据可以处理时，操作系统会通知进程，进程从阻塞状态返回，开始进行业务处理。 Reactor 事件反应 : 来了一个事件我就有相应的反应 Reactor模式三种典型实现方案 单 Reactor 单进程 / 线程 Reactor 对象通过 select 监控连接事件，收到事件后通过 dispatch 进行分发 如果是连接建立的事件，则由 Acceptor 处理，Acceptor 通过 accept 接受连接，并创建一个 Handler 来处理连接后续的各种事件 如果不是连接建立事件，则 Reactor 会调用连接对应的 Handler（第 2 步中创建的 Handler）来进行响应。 Handler 会完成 read-> 业务处理 ->send 的完整业务流程。 缺点 只有一个进程，无法发挥多核 CPU 的性能 Handler 在处理某个连接上的业务时，整个进程无法处理其他连接的事件，很容易导致性能瓶颈。 单 Reactor 多线程 主线程中，Reactor 对象通过 select 监控连接事件，收到事件后通过 dispatch 进行分发。 如果是连接建立的事件，则由 Acceptor 处理，Acceptor 通过 accept 接受连接，并创建一个 Handler 来处理连接后续的各种事件 如果不是连接建立事件，则 Reactor 会调用连接对应的 Handler（第 2 步中创建的 Handler）来进行响应。 Handler 只负责响应事件，不进行业务处理；Handler 通过 read 读取到数据后，会发给 Processor 进行业务处理。 Processor 会在独立的子线程中完成真正的业务处理，然后将响应结果发给主进程的 Handler 处理；Handler 收到响应后通过 send 将响应结果返回给 client。 缺点 多线程数据共享和访问比较复杂 Reactor 承担所有事件的监听和响应，只在主线程中运行，瞬间高并发时会成为性能瓶颈。 单 Reactor 多进程 单 Reactor 多进程的复杂度很高, 不宜使用. 多 Reactor 多进程 / 线程 父进程中 mainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 接收，将新的连接分配给某个子进程 子进程的 subReactor 将 mainReactor 分配的连接加入连接队列进行监听，并创建一个 Handler 用于处理连接的各种事件 当有新的事件发生时，subReactor 会调用连接对应的 Handler（即第 2 步中创建的 Handler）来进行响应Handler 完成 read→业务处理→send 的完整业务流程 案例 Nginx 采用的是多 Reactor 多进程Nginx 采用的是多 Reactor 多进程的模式，但方案与标准的多 Reactor 多进程有差异。具体差异表现为主进程中仅仅创建了监听端口，并没有创建 mainReactor 来“accept”连接，而是由子进程的 Reactor 来“accept”连接，通过锁来控制一次只有一个子进程进行“accept”，子进程“accept”新连接后就放到自己的 Reactor 进行处理，不会再分配给其他子进程 Memcache 和 Netty 采用的是多 Reactor 多线程的实现 Proactor Reactor 是非阻塞同步网络模型，因为真正的 read 和 send 操作都需要用户进程同步操作。这里的“同步”指用户进程在执行 read 和 send 这类 I/O 操作的时候是同步的 主动器: 来了事件我来处理，处理完了我通知你 Proactor方案 Proactor Initiator 负责创建 Proactor 和 Handler，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核 Asynchronous Operation Processor 负责处理注册请求，并完成 I/O 操作 Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理 Handler 完成业务处理，Handler 也可以注册新的 Handler 到内核进程 总结理论上 Proactor 比 Reactor 效率要高一些，异步 I/O 能够充分利用 DMA 特性，让 I/O 操作与计算重叠，但要实现真正的异步 I/O，操作系统需要做大量的工作目前 Windows 下通过 IOCP 实现了真正的异步 I/O，而在 Linux 系统下的 AIO 并不完善，因此在 Linux 下实现高并发网络编程时都是以 Reactor 模式为主。 高性能负载均衡 由于计算本身存在一个特点：同样的输入数据和逻辑，无论在哪台服务器上执行，都应该得到相同的输出。因此高性能集群设计的复杂度主要体现在任务分配这部分高性能集群的复杂性主要体现在需要增加一个任务分配器，以及为任务选择一个合适的任务分配算法 负载均衡不只是为了计算单元的负载达到均衡状态 分类与架构 DNS 负载均衡 DNS 是最简单也是最常见的负载均衡方式，一般用来实现地理级别的均衡 优点 简单、成本低 就近访问，提升访问速度 缺点 更新不及时 扩展性差 分配策略比较简单 针对 DNS 负载均衡的一些缺点, 可以自己实现HTTP-DNS 的功能，即使用 HTTP 协议实现一个私有的 DNS 系统。这样的方案和通用的 DNS 优缺点正好相反。 硬件负载均衡 硬件负载均衡是通过单独的硬件设备来实现负载均衡功能，这类设备和路由器、交换机类似，可以理解为一个用于负载均衡的基础网络设备 优点 功能强大 全面支持各层级的负载均衡，支持全面的负载均衡算法，支持全局负载均衡 性能强大 对比一下，软件负载均衡支持到 10 万级并发已经很厉害了，硬件负载均衡可以支持 100 万以上的并发 稳定性高 支持安全防护 具备防火墙、防 DDoS 攻击等安全功能 缺点 价格昂贵 扩展能力差 软件负载均衡 软件负载均衡通过负载均衡软件来实现负载均衡功能 优点 简单 便宜 灵活 缺点 性能一般 功能没有硬件负载均衡那么强大 一般不具备防火墙和防 DDoS 攻击等安全功能 这几种负载均衡机制可以基于他们的优缺点进行组合使用 组合基本原则 DNS 负载均衡用于实现地理级别的负载均衡 硬件负载均衡用于实现集群级别的负载均衡 软件负载均衡用于实现机器级别的负载均衡 一般只有大型业务场景才需要三种组合使用, 一般的场景可能只需要一个nginx作为简单的负载均衡即可 算法 分为以下几类 任务平分类 负载均衡系统将收到的任务平均分配给服务器进行处理，这里的“平均”可以是绝对数量的平均，也可以是比例或者权重上的平均 负载均衡类 负载均衡系统根据服务器的负载来进行分配，这里的负载并不一定是通常意义上我们说的“CPU 负载”，而是系统当前的压力，可以用 CPU 负载来衡量，也可以用连接数、I/O 使用率、网卡吞吐量等来衡量系统的压力 性能最优类 负载均衡系统根据服务器的响应时间来进行任务分配，优先将新任务分配给响应最快的服务器 Hash 类 负载均衡系统根据任务中的某些关键信息进行 Hash 运算，将相同 Hash 值的请求分配到同一台服务器上 轮询 负载均衡系统收到请求后，按照顺序轮流分配到服务器上轮询是最简单的一个策略，无须关注服务器本身的状态, 只要服务器在运行，运行状态是不关注的. 但如果服务器直接宕机了，或者服务器和负载均衡系统断连了，这时负载均衡系统是能够感知的，也需要做出相应的处理 加权轮询 解决不同服务器处理能力有差异的问题如: 有的机器性能好, 可以加高权重. 负载最低优先 负载均衡系统将任务分配给当前负载最低的服务器，这里的负载根据不同的任务类型和业务场景，可以用不同的指标来衡量 LVS 这种 4 层网络负载均衡设备，可以以“连接数”来判断服务器的状态，服务器连接数越大，表明服务器压力越大 Nginx 这种 7 层网络负载系统，可以以“HTTP 请求数”来判断服务器状态（Nginx 内置的负载均衡算法不支持这种方式，需要进行扩展） 如果我们自己开发负载均衡系统，可以根据业务特点来选择指标衡量系统压力。如果是 CPU 密集型，可以以“CPU 负载”来衡量系统压力；如果是 I/O 密集型，可以以“I/O 负载”来衡量系统压力 负载最低优先的算法解决了轮询算法中无法感知服务器状态的问题，由此带来的代价是复杂度要增加很多 如果负载均衡系统和服务器之间是固定的连接池方式，就不适合采取最少连接数优先的算法 CPU 负载最低优先的算法要求负载均衡系统以某种方式收集每个服务器的 CPU 负载, 这种收集的时间间隔又比较难以确定(太长容易造成峰值带来的临时相应缓慢, 太短容易造成频繁波动) 负载最低优先算法如果本身没有设计好，或者不适合业务的运行特点，算法本身就可能成为性能的瓶颈 性能最优类 负载最低优先类算法是站在服务器的角度来进行分配的，而性能最优优先类算法则是站在客户端的角度来进行分配的，优先将任务分配给处理速度最快的服务器，通过这种方式达到最快响应客户端的目的 和负载最低优先类算法类似，性能最优优先类算法本质上也是感知了服务器的状态，只是通过响应时间这个外部标准来衡量服务器状态而已 负载均衡系统需要收集和分析每个服务器每个任务的响应时间，在大量任务处理的场景下，这种收集和统计本身也会消耗较多的性能 为了减少这种统计上的消耗，可以采取采样的方式来统计，即不统计所有任务的响应时间，而是抽样统计部分任务的响应时间来估算整体任务的响应时间 无论是全部统计还是采样统计，都需要选择合适的周期 没有放之四海而皆准的周期，需要根据实际业务进行判断和选择，这也是一件比较复杂的事情，甚至出现系统上线后需要不断地调优才能达到最优设计 Hash类 负载均衡系统根据任务中的某些关键信息进行 Hash 运算，将相同 Hash 值的请求分配到同一台服务器上 源地址 Hash 将来源于同一个源 IP 地址的任务分配给同一个服务器进行处理，适合于存在事务、会话的业务 ID Hash 将某个 ID 标识的业务分配到同一个服务器中进行处理，这里的 ID 一般是临时性数据的 ID（如 session id） Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/ha/cap.html":{"url":"docs/ha/cap.html","title":"CAP","keywords":"","body":"CAPCAP的不同观点第一版解释第二版解释CAP详细解释CAP关键细节ACIDBASECAP CAP的不同观点 第一版解释 对于一个分布式计算系统，不可能同时满足一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三个设计约束。 第二版解释 在一个分布式系统（指互相连接并共享数据的节点的集合）中，当涉及读写操作时，只能保证一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三者中的两个，另外一个必须被牺牲。 CAP详细解释 一致性 版本一对某个指定的客户端来说，读操作保证能够返回最新的写操作结果 版本二所有节点在同一时刻都能看到相同的数据 第一版从节点 node 的角度描述，第二版从客户端 client 的角度描述第一版的关键词是 see，第二版的关键词是 read。第一版强调同一时刻拥有相同数据（same time + same data），第二版并没有强调这点。 可用性 版本一每个请求都能得到成功或者失败的响应。 版本二非故障的节点在合理的时间内返回合理的响应（不是错误和超时的响应）。 第一版是 every request，第二版强调了 A non-failing node。第一版的 response 分为 success 和 failure，第二版用了两个 reasonable：reasonable response 和 reasonable time，而且特别强调了 no error or timeout 分区容错性 版本一出现消息丢失或者分区错误时系统能够继续运行。 版本二当出现网络分区后，系统能够继续“履行职责”。 第一版用的是 work，第二版用的是 function。work 强调“运行”，只要系统不宕机，我们都可以说系统在 work，返回错误也是 work，拒绝服务也是 work；而 function 强调“发挥作用”“履行职责”，这点和可用性是一脉相承的。也就是说，只有返回 reasonable response 才是 function。 第一版描述分区用的是 message loss or partial failure，第二版直接用 network partitions。 CAP关键细节 CAP 关注的粒度是数据，而不是整个系统 C 与 A 之间的取舍可以在同一系统内以非常细小的粒度反复发生，而每一次的决策可能因为具体的操作，乃至因为牵涉到特定的数据或用户而有所不同。 CAP 是忽略网络延迟的 正常运行情况下，不存在 CP 和 AP 的选择，可以同时满足 CA 既要考虑分区发生时选择 CP 还是 AP，也要考虑分区没有发生时如何保证 CA 放弃并不等于什么都不做，需要为分区恢复后做准备 CAP 理论的“牺牲”只是说在分区过程中我们无法保证 C 或者 A，但并不意味着什么都不做。因为在系统整个运行周期中，大部分时间都是正常的，发生分区现象的时间并不长 最典型的就是在分区期间记录一些日志，当分区故障解决后，系统根据日志进行数据恢复，使得重新达到 CA 状态 ACID ACID 是数据库管理系统为了保证事务的正确性而提出来的一个理论 Atomicity（原子性） 一个事务中的所有操作，要么全部完成，要么全部不完成，不会在中间某个环节结束。事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。 Consistency（一致性） 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。 Isolation（隔离性） 数据库允许多个并发事务同时对数据进行读写和修改的能力。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 Durability（持久性） 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 BASE BASE 是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency），核心思想是即使无法做到强一致性（CAP 的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性。 基本可用（Basically Available） 分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。 软状态（Soft State） 允许系统存在中间状态，而该中间状态不会影响系统整体可用性。这里的中间状态就是 CAP 理论中的数据不一致。 最终一致性（Eventual Consistency） 系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。 BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充 CAP和BASE相关的两点 CAP 理论是忽略延时的，而实际应用中延时是无法避免的。 这一点就意味着完美的 CP 场景是不存在的，即使是几毫秒的数据复制延迟，在这几毫秒时间间隔内，系统是不符合 CP 要求的 AP 方案中牺牲一致性只是指分区期间，而不是永远放弃一致性。 这一点其实就是 BASE 理论延伸的地方，分区期间牺牲一致性，但分区故障恢复后，系统应该达到最终一致性。 ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/ha/fmea.html":{"url":"docs/ha/fmea.html","title":"FMEA","keywords":"","body":"FMEA在架构设计领域, FMEA具体分析方法常见的 FMEA 分析表功能点故障模式故障影响严重程度故障原因故障概率风险程度已有措施规避措施解决措施后续规划FMEA 故障模式与影响分析FMEA 是一套分析和思考的方法，而不是某个领域的技能或者工具 在架构设计领域, FMEA具体分析方法 给出初始的架构设计图。 假设架构中某个部件发生故障。 分析此故障对系统功能造成的影响。 根据分析结果，判断架构是否需要进行优化 常见的 FMEA 分析表 功能点 这里的“功能点”指的是从用户角度来看的，而不是从系统各个模块功能点划分来看的对于一个用户管理系统，使用 FMEA 分析时 “登录”“注册”才是功能点，而用户管理系统中的数据库存储功能、Redis 缓存功能不能作为 FMEA 分析的功能点。 故障模式 故障模式指的是系统会出现什么样的故障，包括故障点和故障形式这里的故障模式并不需要给出真正的故障原因，我们只需要假设出现某种故障现象即可 故障影响 当发生故障模式中描述的故障时，功能点具体会受到什么影响 严重程度 严重程度指站在业务的角度故障的影响程度，一般分为“致命 / 高 / 中 / 低 / 无”五个档次严重程度 = 功能点重要程度 × 故障影响范围 × 功能点受损程度 故障原因 故障模式”中只描述了故障的现象，并没有单独列出故障原因 不同的故障原因发生概率不相同 不同的故障原因检测手段不一样 不同的故障原因的处理措施不一样 故障概率 指某个具体故障原因发生的概率 硬件 随着时间推移,故障概率会越来越高. 开源系统 成熟的开源系统和新开源系统bug率相差很大 自研系统 成熟的自研系统和新自研系统bug率相差很大 风险程度 风险程度就是综合严重程度和故障概率来一起判断某个故障的最终等级风险程度 = 严重程度 × 故障概率 已有措施 针对具体的故障原因，系统现在是否提供了某些措施来应对，包括：检测告警、容错、自恢复等 检测告警 最简单的措施就是检测故障，然后告警，系统自己不针对故障进行处理，需要人工干预。 容错 检测到故障后，系统能够通过备份手段应对 自恢复 检测到故障后，系统能够自己恢复 规避措施 规避措施指为了降低故障发生概率而做的一些事情，可以是技术手段，也可以是管理手段 技术手段：为了避免新引入的 MongoDB 丢失数据，在 MySQL 中冗余一份 管理手段：为了降低磁盘坏道的概率，强制统一更换服务时间超过 2 年的磁盘 解决措施 解决措施指为了能够解决问题而做的一些事情，一般都是技术手段 为了解决密码暴力破解，增加密码重试次数限制 为了解决拖库导致数据泄露，将数据库中的敏感数据加密保存。 为了解决非法访问，增加白名单控制 后续规划 综合前面的分析，就可以看出哪些故障我们目前还缺乏对应的措施，哪些已有措施还不够，针对这些不足的地方，再结合风险程度进行排序，给出后续的改进规划 地震导致机房业务中断：这个故障模式就无法解决，只能通过备份中心规避，尽量减少影响；而机柜断电导致机房业务中断：可以通过将业务机器分散在不同机柜来规避 敏感数据泄露：这个故障模式可以通过数据库加密的技术手段来解决。 MongoDB 断电丢数据：这个故障模式可以通过将数据冗余一份在 MySQL 中，在故障情况下重建数据来规避影响。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/ha/store.html":{"url":"docs/ha/store.html","title":"高可用存储架构","keywords":"","body":"高可用存储架构双机架构主备复制主从复制双机切换主主复制集群和分区数据集群数据分区高可用存储架构 双机架构 常见的双机高可用架构：主备、主从、主备 / 主从切换和主主。 主备复制 主备复制是最常见也是最简单的一种存储高可用方案，几乎所有的存储系统都提供了主备复制的功能，例如 MySQL、Redis、MongoDB 等。 基本实现其整体架构比较简单，主备架构中的“备机”主要还是起到一个备份作用，并不承担实际的业务读写操作，如果要把备机改为主机，需要人工操作。 优点 对于客户端来说，不需要感知备机的存在，即使灾难恢复后，原来的备机被人工修改为主机后，对于客户端来说，只是认为主机的地址换了而已，无须知道是原来的备机升级为主机 对于主机和备机来说，双方只需要进行数据复制即可，无须进行状态判断和主备切换这类复杂的操作 缺点 备机仅仅只为备份，并没有提供读写操作，硬件成本上有浪费 故障后需要人工干预，无法自动恢复 综合主备复制架构的优缺点，内部的后台管理系统使用主备复制架构的情况会比较多，例如学生管理系统、员工管理系统、假期管理系统等，因为这类系统的数据变更频率低，即使在某些场景下丢失数据，也可以通过人工的方式补全。 主从复制 主机负责读写操作，从机只负责读操作，不负责写操作。 基本实现与主备复制架构比较类似，主要的差别点在于从机正常情况下也是要提供读的操作 优点主从复制在主机故障时，读操作相关的业务可以继续运行主从复制架构的从机提供读操作，发挥了硬件的性能 缺点主从复制架构中，客户端需要感知主从关系，并将不同的操作发给不同的机器进行处理，复杂度比主备复制要高主从复制架构中，从机提供读业务，如果主从复制延迟比较大，业务会因为数据不一致出现问题故障时需要人工干预 综合主从复制的优缺点，一般情况下，写少读多的业务使用主从复制的存储架构比较多。例如，论坛、BBS、新闻网站这类业务，此类业务的读操作数量是写操作数量的 10 倍甚至 100 倍以上 双机切换 主备复制和主从复制方案存在两个共性的问题 主机故障后，无法进行写操作 如果主机无法恢复，需要人工指定新的主机角色 设计关键点 主机间状态判断 状态传递的渠道 状态检测的内容 切换决策 切换时机什么情况下备机应该升级为主机？是机器掉电后备机才升级，还是主机上的进程不存在就升级，还是主机响应时间超过 2 秒就升级，还是 3 分钟内主机连续重启 3 次就升级等 切换策略 原来的主机故障恢复后，要再次切换，确保原来的主机继续做主机，还是原来的主机故障恢复后自动成为新的备机？ 自动程度 数据冲突解决 当原有故障的主机恢复后，新旧主机之间可能存在数据冲突 常见架构 互联式 指主备机直接建立状态传递的渠道 中介式 在主备两者之外引入第三方中介，主备机之间不直接连接，而都去连接中介，并且通过中介来传递状态信息 优点 连接管理更简单 主备机无须再建立和管理多种类型的状态传递连接通道，只要连接到中介即可，实际上是降低了主备机的连接管理复杂度 状态决策更简单 主备机的状态决策简单了，无须考虑多种类型的连接通道获取的状态信息如何决策的问题，只需要按照下面简单的算法即可完成状态决策 无论是主机还是备机，初始状态都是备机，并且只要与中介断开连接，就将自己降级为备机，因此可能出现双备机的情况 主机与中介断连后，中介能够立刻告知备机，备机将自己升级为主机 如果是网络中断导致主机与中介断连，主机自己会降级为备机，网络恢复后，旧的主机以新的备机身份向中介上报自己的状态 主备机与中介连接都正常的情况下，按照实际的状态决定是否进行切换 中介本身需要高可用ZooKeeper 和 Keepalived是比较成熟开源的中介式解决方案 模拟式 模拟式指主备机之间并不传递任何状态数据，而是备机模拟成一个客户端，向主机发起模拟的读写操作，根据读写操作的响应情况来判断主机的状态优点是简单, 无需建立状态传递通道和管理工作缺点是响应状态有限(只有响应时间,404等,而互联式可以有cpu,io,吞吐量等状态),可能出现偏差 主主复制 两台机器都是主机，互相将数据复制给对方，客户端可以任意挑选其中一台机器进行读写操作 优点 两台都是主机，不存在切换的概念 客户端无须区分不同角色的主机，随便将读写操作发送给哪台主机都可以 缺点如果采取主主复制架构，必须保证数据能够双向复制，而很多数据是不能双向复制的 用户注册后生成的用户 ID，如果按照数字增长，那就不能双向复制，否则就会出现 X 用户在主机 A 注册，分配的用户 ID 是 100，同时 Y 用户在主机 B 注册，分配的用户 ID 也是 100，这就出现了冲突 库存不能双向复制。例如，一件商品库存 100 件，主机 A 上减了 1 件变成 99，主机 B 上减了 2 件变成 98，然后主机 A 将库存 99 复制到主机 B，主机 B 原有的库存 98 被覆盖，变成了 99，而实际上此时真正的库存是 97。类似的还有余额数据。 主主复制架构对数据的设计有严格的要求，一般适合于那些临时性、可丢失、可覆盖的数据场景。例如，用户登录产生的 session 数据（可以重新登录生成）、用户行为的日志数据（可以丢失）、论坛的草稿数据（可以丢失）等 集群和分区 数据集群 数据集中集群 数据集中集群与主备、主从这类架构相似，可以称数据集中集群为1主多备或1主多从。 问题 主机如何将数据复制给备机 与主备架构相比，数据集中集群架构存在多条复制通道。会增加主机的复制压力。 多条复制通道可能会导致多个备机之间数据不一致，所以还需要对备机之间的数据一致性进行检查和修正。 备机如何检测主机状态多台备机都需要对主机状态进行判断，而不同的备机判断的状态结果可能是不同的，如何处理不同备机对主机状态的不同判断，是一个复杂的问题。 主机故障后，如何决定新的主机多台备机都可以升级为主机，但实际上只能允许一台备机升级为主机，备机之间的如何协调，是一个复杂的问题。 目前开源的数据集中集群以ZooKeeper为典型，ZooKeeper通过ZAB算法来解决上述的问题，但ZAB算法的复杂的很高。 数据分散集群 数据分散集群指多个服务器组成一个集群，每台服务器都会负责存储一部分数据；同时为了提升硬件利用率，每台服务器又会备份一部分数据。数据分散集群的复杂点在于如何将数据分配到不同的服务器上 考虑点 均衡性 保证服务器上的数据分区基本是均衡的，不能存在某台服务器上的分区数量是另外一台服务器得几倍的情况。 容错性 当出现部分服务器故障时，算法需要将原来分配给故障服务器的数据分区分配给其他服务器。 可伸缩性当集群数量不够，扩充新的服务器后，算法能够自动将部分数据分区迁移到新服务器，并保证扩容后所有服务器的均衡性。 数据分散集群和数据集中集群不同点在于，数据分散集群中的每台服务器都可以处理读和写请求。在数据分散集群中，必须有一个角色来负责执行数据分配算法（可以是独立服务器，也可以是集群自己选举一个）。 Hadoop的实现是独立服务器负责数据分区的分配，这台服务器叫做Namenode。 数据集中集群适合数据量不大，集群机器数量不多的场景。（ZooKeeper集群） 数据分散集群，由于其良好的可伸缩性，适合业务数据量巨大、集群机器数量庞大的业务场景。（Hadoop集群、HBase集群） 分布式事物算法 2PC（二阶段提交） 在分布式系统中，存在一个节点作为协调者(Corordinator)，其他节点作为参与者(Cohorts)，且节点之间可以进行通信。 所有节点都采用预写日志，且日志被写入后既保持在可靠的存储设备上，即使节点损坏，也不会导致日志数据的消失。 所有节点不会永久损坏，即使损坏，仍然可以恢复。 第一阶段 协调者向所有参与者发送QUERY TO COMMIT消息，询问是否可以执行提交事物，并开始等待各参与者的响应。 参与者执行询问发起为止的所有事物操作，并将Undo信息和Redo信息写入日志，返回yes消息给协调者，如果参与者执行失败，则返回no消息给协调者。 第一阶段也可以被称为投票阶段。 第二阶段 成功协调者从所有参与者获得的相应消息都为yes时 协调着向所有参与者发出COMMIT的请求 参与者完成COMMIT操作，并释放在整个事物期间占用的资源 参与者向协调者发送ACK消息 协调者收到所有参与者反馈的ACK消息后，完成事物。 失败存在参与者在第一阶段返回响应消息为no，或协调者在第一阶段的询问超时之前无法获取所有参与者的响应消息。 协调者向所有参与者发出ROLLBACK的请求。 参与者利用之前写入的Undo信息执行回滚，并释放在整个事物期间占用的资源 参与者向协调者发送ACK消息 协调者收到所有参与者消息反馈ACK消息后，取消事物。 第二阶段也可以被称作完成阶段，无论结果怎样，协调者都必须在此阶段结束当前事物。 特点 同步阻塞 整个过程中，协调者和参与者互相等待对方的响应消息，等待过程中节点处于阻塞状态，不能做其他事情。 状态不一致 第二阶段执行过程中，如果协调者发出commit请求消息后，而参与者并没有完全收到该消息，那么收到该消息的参与者会提交事物，未收到的参与者超时后会回滚事物，导致事物状态不一致。 虽然协调者在这种情况下可以再发送ROLLBACK消息给参与者，但这条消息一样存在丢失问题，所以在极端起给你看下，无论如何都可能出现状态不一致的情况。 单点故障 协调者是整个算法的单点，如果协调者故障，则参与者会一直阻塞下去。 3PC（三阶段提交） 第一阶段 协调者向参与者发送canCommit消息，询问参与者是否可以提交事物 参与者收到canCommit消息后，判别自己是否可以提交该事物，如果可以执行就返回yes，否则返回no 如果协调者收到任何一个no或者参与者超时，则事物终止，同时会通知参与者终止事物，如果在超时时间内收到所有yes，则进入第二阶段 第二阶段 协调者发送preCommit消息给所有参与者，告诉参与者准备提交 参与者收到preCommit消息后，执行事物操作，将undo和redo信息记录到事物日志中，然后返回ACK消息 第三阶段 协调者在接收到所有ACK消息后发送doCommit，告诉参与者正式提交；否则会给参与者发送终止消息，事物回滚 参与者收到doCommit消息后提交事物，然后返回haveCommitted消息 如果参与者收到一个preCommit消息并返回了ACK，但等待doCommit消息超时，参与者则会在超时后继续提交事物 同样存在数据不一致问题。 数据分区 数据分区指将数据按照一定规则进行分区，不同分区分布在不同的地理位置上，每个分区存储一部分数据，通过这种方式来规避地理级别的故障造成的巨大影响。 考虑点 数据量 数据量越大，分区规则会越复杂，考虑点情况也越多。 分区规则 洲际分区、国家分区、城市分区。综合考虑业务范围、成本因素进行选择。 复制规则 每个分区的数据量可能还是很大的，如果分区的数据损坏或者丢失，同样难以接受。所以分区架构也要考虑复制方案。 集中式 存在一个总的备份中心，所有的分区都将数据备份到备份中心。 特点 设计简单，各分区之间并无直接联系，可以做到互不影响 扩展容易，增加新的分区，只需要将新的分区的数据复制到备份中心即可。 成本较高 互备式 每个分区备份另外一个分区的数据。 特点 设计比较复杂，各分区除了承担业务数据存储，还要承担备份功能，相互之间互相关联和影响。 扩展麻烦 成本低 独立式 每个分区自己有独立的备份中心。 设计简单，各分区互不影响。 扩展容易，新增加的分区只需要搭建自己的备份中心即可。 成本高。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/ha/calcul.html":{"url":"docs/ha/calcul.html","title":"计算高可用架构","keywords":"","body":"计算高可用架构哪些服务器可以执行任务任务如何从新执行常见的计算高可用架构主备主从集群计算高可用架构 计算高可用架构的设计复杂度主要体现在任务管理方面，即当任务在某台服务器上执行失败后，如何将任务从新分配到新的服务器进行执行。 哪些服务器可以执行任务 每个服务器都可以执行任务（与计算高性能中断集群类似） 例如：常见的访问网站的某个页面。 只有特定服务器可以执行任务（与存储高可用中的集群类似） 当执行任务的服务器故障后，系统需要挑选新的服务器来执行。 例如：ZooKeeper的Leader才能处理写操作请求。 任务如何从新执行 对于已经分配的任务即使执行失败也不做任何处理，系统只需要保证新的任务能够分配到其他非故障服务器上执行即可。 设计一个任务管理器来管理需要执行的计算任务，服务器执行完成任务后，需要向任务管理器反馈任务执行结果，任务管理器根据任务执行结果来决定是否需要将任务重新分配到另外的服务器上执行。 任务分配器是一个逻辑上的概念，并不一定要求系统存在一个独立任务分配器模块。 常见的计算高可用架构 主备 和存储高可用主备复制架构类似，但是计算高可用主备架构无须数据复制，相对更简单。 设计 主机执行所有计算任务 主机故障时，任务分配器不会自动将计算任务发送给备机，此时系统处于不可用状态。 如果主机能够恢复，任务分配器继续将任务发送给主机。 如果主机不能够恢复，则需要人工人工操作，将备机升为主机，然后让任务分配器将任务发送给新的主机；同时，需要人工增加新的机器作为备机。 根据备机状态不同，主备架构可以细分为冷备架构和温备架构 冷备备机上的程序包和配置文件都准备好，但备机上的业务系统没有启动，主机故障后，需要人工手工将备机的业务系统启动，并将任务分配器的任务请求切换发送给备机。 温备备机上的业务系统已经启动，只是不对外提供服务，主机故障后，人工只需将任务分配器的任务请求切换发送到备机即可。 主备架构优点就是简单，但是人工切换时需要花费很久的时间。和存储高可用中的主备复制架构类似，计算高可用的主备架构也比较适合与内部管理系统、后台管理系统这类使用人数不多、使用频率不高的业务。 主从 计算高可用的主从架构中的从机也需要执行任务。任务分配器需要将任务进行分类，确定哪些任务可以发送给主机执行，哪些任务可以发送给备机执行。 设计 正常情况下，主机执行部分计算任务，备机执行部分计算任务 当主机故障时，任务分配器不会自动将原本发送给主机的任务发送给从机，而是继续发送给主机，不管这些任务执行是否成功 如果主机能够恢复，任务分配器继续按照原有的设计策略分配任务 如果主机不能够恢复，则需要人工操作，将原来的从机升级为主机，增加新的机器作为从机，新的从机准备就绪后，任务分配器继续按照原有的设计策略分配任务 主机架构的从机也执行任务，发挥了从机的硬件性能。主机架构需要将任务分类，任务分配器会复杂一些。 集群 对称集群 对称集群通俗叫法“负载均衡集群” 设计 正常情况下，任务分配器采取某种策略将计算任务分配给集群中的不同服务器 当集群中的某太服务器故障后，任务分配器不再将任务分配给它，而是将任务分配给其他服务器执行 当故障的服务器恢复后，任务分配器从新将任务分配给他执行 关键点 任务分配器需要选取分配策略 任务分配器需要检测服务器状态任务分配策略使用轮询后者随机基本就够了.状态检测需要检测服务器端状态（宕机、网络等）；同时还要检测任务的执行状态（任务卡死、执行时间过长等）。一般任务分配器和服务器之间通过心跳来传递信息，包括服务器信息和任务信息。 非对称集群 非对称集群中的服务器的角色是不同的，不同角色的服务器承担不同的职责。以Master－Slave为例，部分任务是Master服务器才能执行，部分任务是Slave服务器才能执行。 设计 集群会通过某种方式来区分不同服务器的角色。（ZAP算法选举等） 任务分配器将不同任务发送给不同服务器。 当指定类型服务器故障时，需要重新分配角色。 关键点 任务分配器策略很复杂：需要将任务划分为不同类型并分配给不同角色集群节点。 角色分配策略实现比较复杂： 例如，可能需要使用ZAB、Raft这类复杂度算法来实现Leader选举。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/ha/difflive.html":{"url":"docs/ha/difflive.html","title":"异地多活架构","keywords":"","body":"异地多活架构异地多活标准异地多活代价架构模式异地多活设计4大技巧异地多活四步业务分级数据分类数据同步异常处理应对接口级故障原因解决方案异地多活架构 异地多活标准 正常情况下，用户无论访问哪一个地点的业务系统，都能够得到正确的业务服务。 某个地方业务异常的时候，用户访问其他地方正常的业务系统，能够得到正确的业务服务。 异地多活代价 系统复杂度发生质的变化，需要设计复杂的异地多活架构。 成本上升，毕竟要多在一个或多个机房搭建独立的一套业务系统。 架构模式 同城异区 将业务部署在同一个城市的不同区的多个机房。 特点 在逻辑上可以看做同一个机房（距离近，延迟低，可以视为同一个机房），降低了设计复杂度。 机房级别故障可以很好的解决（机房停电、机房火灾等）。 跨城异地 业务部署在不同城市的多个机房（距离最好远一些） 特点 可以解决区域灾难（如区域水灾、地震、光缆挖断等）. 距离远才能更好的避免这类灾害. 网络延迟太高，给异地多活架构带来了复杂性。 会有数据一致性问题 跨国异地 业务部署在不同国家的多个机房。 跨国异地因为延迟很高，使用场景有限。 应用场景 为不同地区用户提供服务 例如：亚马逊中国为中国用户服务，亚马逊美国为美国用户服务，亚马逊中国用户如果访问美国亚马逊，是无法用亚马逊中国的账号登陆美国亚马逊的。 只读类业务做多活 例如：谷歌的搜索业务，由于用户搜索资料时，这些资料都已经存在于谷歌的搜索引擎上面，无论是访问英国谷歌，还是美国谷歌，搜索结果基本相同，对于用户来说，也不需要搜索到最新的实时资料，跨国异地的几秒钟延迟，对搜索结果几乎没有影响。 异地多活设计4大技巧 保证核心业务的异地多活 在很多情况下，要保证全部业务支持异地多活是很难的，可以优先实现核心业务的异地多活架构。 保证核心数据最终一致性 异地多活的本质是通过异地的数据冗余，来保证在极端异常的情况下业务也能够正常提供给用户，因此数据同步是异地多活架构设计的核心。 解决方案 尽量减少异地多活机房的距离，搭建高速网络 异城搭建高速网络成不巨大。 尽量减少数据同步，只同步核心业务相关的数据 某些数据不同步，可能会影响用户体验。 保证最终一致性，不保证实时一致性 采用多种手段同步数据 消息队列方式 对于账号数据，由于账号只会创建，不会更改和删除，可以将账号通过消息队列同步到其他业务中心。 二次读取方式 某些情况下消息队列同步也会延迟；如果用户在A中心注册，然后去访问B中心的业务，此时B中心本地拿不到用户的账号数据。B中心读取本地数据失败时，可以根据路由规则，再去A中心访问一次。 存储系统同步方式 对于密码数据，用户改动频率较低，可以通过数据库的同步机制将数据复制到其他业务中心即可。 回源读取方式 对于登录的session数据，由于数据量很大，可以不同步；当用户在A中心登录后，又在B中心登录，B中心拿到用户上传的session id后，根据路由判断session属于A中心，直接去A中心请求session数据即可。 重新生成数据方式 对于“回源读取”场景，如果异常情况下，A中心宕机，B中心请求session数据失败，此时就只能登录失败，让用户重新在B中心登录，生成新的session数据。 只保证绝大部分用户的异地多活 不能为了保证绝少部分用户而导致全部用户都保证不了。所以，在简单的情况下，尽量保证绝大部分用户能够正常访问业务。 安抚或补偿 挂公告 事后对用户补偿 补充体验 对于为了做异地多活而带来的体验损失，可以想一些方法减少或者规避。 核心思想: 采用多种手段，保证绝大部分用户的核心业务异地多活 异地多活四步 业务分级 常见分级标准 访问量大的业务 核心业务 产生大量收入的业务 数据分类 常见的数据特征分析维度 数据量 这里的数据量包括总的数据量和新增、修改、删除的量 对异地多活架构来说，新增、修改、删除的数据就是可能要同步的数据，数据量越大，同步延迟的几率越高 唯一性 唯一性指数据是否要求多个异地机房产生的同类数据必须保证唯一 数据的唯一性影响业务的多活设计，如果数据不需要唯一，那就说明两个地方都产生同类数据是可能的；如果数据要求必须唯一，要么只能一个中心点产生数据，要么需要设计一个数据唯一生成的算法 实时性 实时性指如果在 A 机房修改了数据，要求多长时间必须同步到 B 机房，实时性要求越高，对同步的要求越高，方案越复杂 可丢失性 可丢失性指数据是否可以丢失。 例如，写入 A 机房的数据还没有同步到 B 机房，此时 A 机房机器宕机会导致数据丢失，那这部分丢失的数据是否对业务会产生重大影响。 可恢复性 可恢复性指数据丢失后，是否可以通过某种手段进行恢复，如果数据可以恢复，至少说明对业务的影响不会那么大，这样可以相应地降低异地多活架构设计的复杂度 数据同步 常见的数据同步方案 存储系统同步 消息队列同步 常见的消息队列有 Kafka、ActiveMQ、RocketMQ 等 消息队列同步适合无事务性或者无时序性要求的数据 重复生成 数据不同步到异地机房，每个机房都可以生成数据，这个方案适合于可以重复生成的数据 异常处理 目的 问题发生时，避免少量数据异常导致整体业务不可用。 问题恢复后，将异常的数据进行修正。 对用户进行安抚，弥补用户损失 措施 多通道同步 多通道同步的含义是采取多种方式来进行数据同步，其中某条通道故障的情况下，系统可以通过其他方式来进行同步，这种方式可以应对同步通道处故障的情况 设计关键点 一般情况下，采取两通道即可，采取更多通道理论上能够降低风险，但付出的成本也会增加很多。 数据库同步通道和消息队列同步通道不能采用相同的网络连接，否则一旦网络故障，两个通道都同时故障；可以一个走公网连接，一个走内网连接。 需要数据是可以重复覆盖的，即无论哪个通道先到哪个通道后到，最终结果是一样的。例如，新建账号数据就符合这个标准，而密码数据则不符合这个标准 同步和访问结合 这里的访问指异地机房通过系统的接口来进行数据访问。 例如业务部署在异地两个机房 A 和 B，B 机房的业务系统通过接口来访问 A 机房的系统获取账号信息 设计关键点 接口访问通道和数据库同步通道不能采用相同的网络连接 数据有路由规则，可以根据数据来推断应该访问哪个机房的接口来读取数据 由于有同步通道，优先读取本地数据，本地数据无法读取到再通过接口去访问，这样可以大大降低跨机房的异地接口访问数量，适合于实时性要求非常高的数据 日志记录 日志记录主要用于用户故障恢复后对数据进行恢复，其主要方式是每个关键操作前后都记录相关一条日志，然后将日志保存在一个独立的地方，当故障恢复后，拿出日志跟数据进行对比，对数据进行修复 常见日志保存方式 服务器上保存日志，数据库中保存数据 这种方式可以应对单台数据库服务器故障或者宕机的情况。 本地独立系统保存日志 这种方式可以应对某业务服务器和数据库同时宕机的情况。 日志异地保存 这种方式可以应对机房宕机的情况 用户补偿 无论采用什么样的异常处理措施，都只能最大限度地降低受到影响的范围和程度，无法完全做到没有任何影响。 常见的补偿措施有送用户代金券、礼包、礼品、红包等 应对接口级故障 接口级故障表现：系统没有宕机，网络没有中断，但业务出现了问题。如：业务响应缓慢、大量访问超时、当量访问出现异常等。这类问题主要原因在于系统压力太大、负载太高，导致无法快速处理业务请求。 原因 内部原因 程序bug导致死循环，某个接口导致查询数据库慢，程序逻辑不完善导致耗尽内存等。 外部原因 黑客攻击、促销或抢购引入超出平时很多倍的用户，第三方系统大量请求，第三方系统响应缓慢等。 解决方案 解决核心思想：优先保证核心业务和优先保证绝大部分用户 降级 将某些业务或接口的功能降低，可以只提供部分功能，也可以是完全停掉所有功能。常见降级方式 系统后门降级 系统预留后门用于降级操作 实现成本低，但如果服务器数量多，需要一台一台操作，效率低。 独立降级系统 将降级操作 独立到一个单独的系统中，可以实现复杂的权限管理、批量操作等功能。 熔断 降级的目的是应对系统自身的故障，熔断的目的是应对依赖的外部系统故障的情况。 假设：A服务的X功能依赖B服务的某个接口，当B的接口响应很慢时，A服务的X功能也会被拖慢，进一步导致A服务被卡在X功能处理上，这时A的其他功能都会被卡住或者响应慢。这时候就需要熔断机制。 既A服务不再请求B服务的这个接口，A服务只要发现是请求B服务的这个接口就立即返回错误。 熔断机制的实现需要一个统一的API调用层，由API调用层来进行采样或统计（如果接口调用散落在代码各处就没法进行统一处理） 限流 降级是从系统功能优先级的角度考虑，限流从用户访问压力的角度考虑。限流指只允许系统能够承受的访问量进来，超出系统访问能力的请求将被丢弃。 限流方式 基于请求限流 从外部访问的请求角度考虑限流（限制总量、限制时间量） 限制总量是限制某个指标的累积上限（如某个直播间限制总用户数量100万） 限制时间量指限制一段时间内某个指标的上限（如1分钟只允许1000个用户访问、每秒峰值最高10万） 特点 实现简单，但是难以找到合适的阈值。 根据阈值来限制访问量的方式适合于业务比较简单的系统（如负载均衡系统、网关系统、抢购系统等） 基于资源限流 从系统内部考虑限流（如cpu占用、连接数、内存占用等达到某个指标限制） 特点 相比于基于请求限流更能有效的反映当前系统的压力，但确定关键资源和确定关键资源的阈值也是需要逐步调优的。 排队 排队是限流的一个变种，限流直接拒绝用户，排队是让用户等待一段时间。 排队需要临时缓存大量的业务请求，单个系统内部无法缓存这么多数据，一般排队需要独立的系统去实现（如使用Kafka这类消息队列来缓存用户请求）。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/he/bt.html":{"url":"docs/he/bt.html","title":"可扩展架构基本思想和模式","keywords":"","body":"可扩展架构基本思想和模式可扩展的基本思想可扩展方式可扩展架构基本思想和模式 软件系统的可扩展特性，魅力体现在可以通过修改和扩展，不断地让软件系统具备更多的功能和特性，满足新的需求或者顺应技术的发展趋势。难点体现在如何以最小的代价去扩展系统，因为很多情况下牵一发而动全身。可扩展性设计主要思考的是如何避免在扩展时改动范围太大。 可扩展的基本思想 可扩展的基本思想：拆 常见拆分思路 面向流程拆分 将整个业务流程拆分为几个阶段，每个阶段作为一部分。 面向服务拆分 将系统提供的服务拆分，每个服务作为一部分。 面向功能拆分 将系统提供的功能拆分，每个功能作为一部分。 范围：流程 > 服务 > 功能 不同的拆分方式，本质上决定了系统的扩展方式。 可扩展方式 面向流程拆分 扩展时大部分情况只需要修改某一层，少部分情况可能修改关联的两层，不会出现所有层都同时修改。 面向服务拆分 某个服务扩展，或者要增加新的服务时，只需要扩展相关的服务即可，无需修改所有的服务。 面向功能拆分 对某个功能扩展，或者要增加新的功能时，只需要扩展相关功能即可，无需修改所有的服务。 典型可扩展系统架构 面向流程拆分：分层架构 面向服务拆分：SOA、微服务 面向功能拆分：微内核架构 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/he/soa.html":{"url":"docs/he/soa.html","title":"传统可扩展架构模式: 分层架构和SOA","keywords":"","body":"传统的可扩展架构模式：分层架构和SOA分层架构SOA传统的可扩展架构模式：分层架构和SOA 分层架构 分层架构也叫N层架构（一般N至少2层）。如：两层：C/S架构、B/S架构。三层：MVC架构、MVP架构。 C/S架构、B/S架构 划分的对象是整个业务系统，划分的纬度是用户交互，既将和用户交互的部分独立为一层，支撑用户交互的后台作为另外一层。 MVC架构、MVP架构 划分的对象是单个业务子系统，划分的纬度是职责，将不同的职责划分为独立层，但各层的依赖关系比较灵活。 逻辑分层架构 划分的对象可以是单个业务子系统，也可以是整个业务系统，划分的纬度是职责。 和MVC架构、MVP架构的不同点在于逻辑分层架构中的层是自顶向下依赖的。 典型的有操作系统内核架构、TCP/IP架构。 核心：需要保证各层之间的差异足够清晰，边界足够明显，让人看到架构图后就能看懂整个架构。 分层之所以能够较好的支撑系统扩展，本质在于隔离关注点，既每个层中的组件只会处理本层逻辑。 分层时需要保证层于层之间的依赖是稳定的，才能支撑快速扩展。 例如：Linux内核为了支撑不同的文件系统格式，抽象了VFS文件系统接口。如果没有VFS，只是简单的将ext2、ext3、reiser等文件系统划分为“文件系统层”，那么这个分层是达不到支撑可扩展的目的的。因为增加一个新的文件系统后，所有基于文件系统的功能都要适配新的文件系统接口；而有了VFS后，只需要VFS适配新的文件系统接口，其他基于文件系统的功能依赖VFS的，不受影响。 分层结构需要层层传递，也就是说一旦分层确定，整个业务流程是按照层进行依次传递的，不能在层之间进行跳跃。这种约束，好处在于强制将分层依赖限定为两两依赖，降低了整体系统复杂度。 SOA SOA全称Service Oriented Archiecture，翻译为“面向服务的架构”。 SOA出现背景是企业内部的it系统重复建设且效率地下 企业各部门都有独立的it系统（如：人力资源系统、财务系统、销售系统），这些系统可能都涉及人员管理，各it系统都需要重复开发人员管理的功能。 如某个员工离职后，需要分别到每个系统中删除员工的权限。 各个独立的it系统可能采购于不同的供应商，实现技术不同。 随着业务发展，复杂度越来越高，更多的流程和业务需要多个it系统合作完成。 由于各个独立的it系统没有标准的实现方式（如人力资源系统用java开发，对外提供rpc协议；而财务系统用c#开发，对外提供soap协议），每次开发新的流程和业务，都需要协调大量的it系统，同时定制开发，效率很低。 SOA应对传统it系统提出三个概念 服务 所有业务功能都是一项服务，服务就意味着要对外提供开放的能力，当其他系统需要使用这项功能时，无须定制开发。 ESB 全称Enterprise Service Bus，翻译为“企业服务总线”。 ESB将企业中各个不同的服务连接在一起。SOA使用ESB来屏蔽异构系统对外提供各个不同的接口方式，以此来达到服务间高效的互联互通。 松耦合 松耦合的目的是减少各个服务间的依赖和互相影响。 采用SOA架构后，各个服务是相互独立运行的，甚至都不清楚某个服务到底有多少对其他服务的依赖。 SOA解决了传统的it系统重复建设和扩展效率低的问题，但其本身引入更多的复杂性。如ESB，需要实现与各个系统间的协议转换、数据转换、透明的动态路由等功能。工作量和复杂度都很大。而且这种转换是需要小号大量的计算性能的，当ESB承载的消息太多时，ESB本身会成为整个系统的性能瓶颈。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/he/microservice.html":{"url":"docs/he/microservice.html","title":"理解微服务架构","keywords":"","body":"深入理解微服务架构微服务和SOA微服务的几个观点微服务和SOA差异微服务的陷阱方法篇基础设施服务发现服务路由服务容错服务监控服务跟踪服务安全自动化测试自动化部署配置中心接口框架API网关搭建微服务基础设施深入理解微服务架构 微服务和SOA 微服务的几个观点 微服务是SOA的实现方式 认为SOA是一种架构理念，而微服务是SOA理念的一种具体实现方法。 如：微服务就是使用HTTP RESTful协议来实现ESB的SOA 微服务是去掉ESB后的SOA 认为传统的SOA架构最为人诟病的就是庞大、复杂、低效的ESB，因此将ESB去掉，改为轻量级的HTTP实现。 微服务是一种和SOA相似但本质上不同的架构理念 两者都关注“服务”，都是通过服务的拆分来解决可扩展性问题。本质上不同的地方在于：是否有ESB、服务的粒度、架构设计的目标等。 微服务和SOA差异 服务粒度 SOA的服务粒度要粗一些，而微服务的服务粒度要细一些。 例如：对于一个大型企业来说，“员工管理系统”就是一个SOA架构中的服务；而如果采用微服务架构，则“员工管理系统”会被拆分成为更多的服务（员工信息管理服务 、员工考勤管理服务等）。 服务通信 SOA采用ESB作为服务间通信的关键组件，负责服务定义、服务路由、消息转换、消息传递，总体上是重量级的实现。 微服务推荐使用统一的协议和格式，如RESTful协议、RPC协议，无须ESB这样重量级的实现。 服务交付 SOA适合于庞大、复杂、异构的企业级系统（这也是SOA诞生的背景）。 这类系统典型的特征就是很多系统已经发展多年，采用不同的企业级技术，有的是内部开发的，有的是外部购买的，无法完全推倒重来或者进行大规模的优化和重构。只能使用ESB来承担兼容的方式进行处理。 微服务适合于快速、轻量级、基于web的互联网系统。 这类系统业务变化快，需要快速尝试、快速交付；同时基本都是基于web，虽然开发技术可能差异很大（如java、c++、。net等），但对外接口基本都是提供HTTP RESTful风格的接口，无须考虑在接口层进行类似SOA的ESB那样的处理。 对比 对比纬度 SOA 微服务 服务粒度 粗 细 服务通信 重量级，ESB 轻量级，如HTTP RESTful 服务交付 慢 快 应用场景 企业级 互联网 SOA和微服务本质上是两种不同的架构设计理念，只是在“服务”这个点上有交集而已，因此两者的关系应该是上面的第三种观点。 微服务的陷阱 服务划分过细，服务间关系复杂 服务划分过细，单个服务的复杂度确实下降了，但整个系统的复杂度却上升了，因为微服务将系统内的复杂度转移为系统间的复杂度了。 服务数量太多，团队效率急剧下降 开发工程师要设计多个接口，打开多个工程，调试时要部署多个程序，提测时要打多个包。 测试工程师要部署多个环境，准备多个微服务的数据，测试多个接口。 运维工程师每次上线都要操作多个微服务，并且微服务之间可能还有依赖关系。 调用链太长，性能下降 由于微服务之间通过HTTP或者RPC调用，每次调用必须经过网络(网络是很慢的，一般线上的业务接口间的调用，平均响应时间大约为50ms)。如果用户的一起请求需要经过6次微服务调用，则性能就是300ms，这在高性能业务场景下难以满足需求。 调用链太长，问题难以定位 系统拆分为微服务后，一次用户请求需要多个微服务协同处理，任意微服务的故障都将导致整个业务失败。然而由于微服务数量较多，且故障存在扩散现象，快速定位到是哪个微服务故障是一件复杂的事情。 没有自动化支撑，无法快速交付 如果没有响应的自动化系统进行支撑，那么微服务不但达不到快速交付的目的，甚至还不如一个大而全的系统效率搞。 没有服务治理，微服务数量多了后管理混乱 服务路由 假设某个微服务有60个节点，部署在20台机器上，那么其他依赖的微服务如何知道这个部署的情况？ 服务故障隔离 假设上述例子中的60个节点有5个节点发生了故障，依赖的微服务如何处理这种情况？ 服务注册和发现 如果决定从60个节点扩容到80个节点或将60个节点缩减为40个节点，新增或减少的节点如何让依赖的服务知道。 解决方案需要依赖自动化服务管理系统，这时就会发现，微服务发展成为了和ESB几乎一样的复杂程度。 方法篇 服务粒度 “三个火枪手”原则。既一个微服务三个人负责开发。 “三个火枪手”原则用于开发阶段。在微服务稳定之后，处于维护期，无须太多的开发，那平均1个人维护1个甚至多个微服务都可以。考虑到人员备份问题，每个微服务最好都安排2个人维护，每个人都可以维护多个微服务。 拆分方法 基于业务逻辑拆分 将系统中的业务模块按照职责范围识别出来，每个单独的业务模块拆分为一个独立的服务。 基于可扩展拆分 将系统中的业务模块按照稳定性排序，将已经成熟和改动不大的服务拆分为稳定服务，将经常变化和迭代的服务拆分为变动服务。稳定的服务粒度可以粗一些，既逻辑上没有强关联的服务，也可以放在同一个子系统中；不稳定的服务粒度可以细一些，但也不要太细，要控制服务的总数量。 这样拆分有助于提升项目快速迭代的效率，避免在开发的时候，影响已有的成熟功能导致线上问题。 基于可靠性拆分 将系统中的业务模块按照优先级排序，将可靠性要求高的核心服务和可靠性要求低的非核心服务拆分开来，重点保证核心服务的高可用。 特点 避免非核心服务故障影响核心服务 核心服务高可用方案可以更简单 能够降低高可用成本 基于性能拆分 将性能要求高或者性能压力大的模块拆分出来，避免性能压力大的服务影响其他服务。 常见的拆分方式和具体的性能瓶颈有关。可以拆分为web服务、数据库、缓存等。 如：电商的抢购，性能压力最大的是入口的排队功能，可以将排队功能独立为一个服务。 多种拆分方式可以排列组合使用。 基础设施 服务发现 我们希望节点的变化能够及时同步到所有其他依赖的微服务。如果采用手工配置，是不可能做到实时更改生效的。因此，需要一套服务发现的系统来支撑微服务的自动注册和发现 实现方式 自理式 自理式结构就是指每个微服务自己完成服务发现 服务自己访问服务注册信息,然后发现其他服务. 代理式 代理式结构就是指微服务之间有一个负载均衡系统，由负载均衡系统来完成微服务之间的服务发现 缺点 一旦负载均衡系统故障，就会影响所有微服务之间的调用 所有的微服务之间的调用流量都要经过负载均衡系统，性能压力会随着微服务数量和流量增加而不断增加，最后成为性能瓶颈 服务路由 有了服务发现后，微服务之间能够方便地获取相关配置信息，但具体进行某次调用请求时，我们还需要从所有符合条件的可用微服务节点中挑选出一个具体的节点发起请求，这就是服务路由需要完成的功能通常情况下是和服务发现放在一起实现的。对于自理式服务发现，服务路由是微服务内部实现的对于代理式服务发现，服务路由是由负载均衡系统实现的。服务路由核心的功能是路由算法。常见的路由算法有：随机路由、轮询路由、最小压力路由、最小连接数路由等。服务容错 服务容错 常见的服务容错包括请求重试、流控和服务隔离。通常情况下，服务容错会集成在服务发现和服务路由系统中 服务监控 作用 实时搜集信息并进行分析，避免故障后再来分析，减少了处理时间。 服务监控可以在实时分析的基础上进行预警，在问题萌芽的阶段发觉并预警，降低了问题影响的范围和时间 通常情况下，服务监控需要搜集并分析大量的数据，因此建议做成独立的系统，而不要集成到服务发现、API 网关等系统中 服务跟踪 服务监控可以做到微服务节点级的监控和信息收集，但如果我们需要跟踪某一个请求在微服务中的完整路径，服务监控是难以实现的。因为如果每个服务的完整请求链信息都实时发送给服务监控系统，数据量会大到无法处理服务监控和服务跟踪的区别可以简单概括为宏观和微观的区别。例如，A 服务通过 HTTP 协议请求 B 服务 10 次，B 通过 HTTP 返回 JSON 对象，服务监控会记录请求次数、响应时间平均值、响应时间最高值、错误码分布这些信息；而服务跟踪会记录其中某次请求的发起时间、响应时间、响应错误码、请求参数、返回的 JSON 对象等信息 关键技术点 标注点 标注点又叫植入点或埋点,通过在应用程序或中间件中明确定义一个全局的标注,它可以是一个特殊的ID,通过这个ID链接每一条记录和发起者的请求,然后跟踪系统再根据这个ID将整个业务请求链串联起来. 采样跟踪 根据一定的概率对请求进行采样跟踪,然后基于采样数据进行分析,可以被用于发现系统问题,但它更通常用于探查性能不足,以及提高全面大规模的工作负载下的系统行为的理解. 染色跟踪 同样的功能或业务,大多数用户都没有问题,很少一部分用户会出错.单从代码逻辑或系统日志很难找到原因,此时需要针对单个用户的特定请求进行全链路跟踪,这就是染色跟踪. 服务安全 为什么需要服务安全从系统连接的角度来说，任意微服务都可以访问所有其他微服务节点；但从业务的角度来说，部分敏感数据或者操作，只能部分微服务可以访问，而不是所有的微服务都可以访问，因此需要设计服务安全机制来保证业务和数据的安全性 接入安全 数据安全 传输安全 服务安全可以集成到配置中心系统中进行实现，即配置中心配置微服务的接入安全策略和数据安全策略，微服务节点从配置中心获取这些配置信息，然后在处理具体的微服务调用请求时根据安全策略进行处理 自动化测试 自动化测试涵盖的范围包括代码级的单元测试、单个系统级的集成测试、系统间的接口测试，理想情况是每类测试都自动化。如果因为团队规模和人力的原因无法全面覆盖，至少要做到接口测试自动化 自动化部署 自动化部署系统包括版本管理、资源管理（例如，机器管理、虚拟机管理）、部署操作、回退操作等功能 配置中心 配置中心包括配置版本管理（例如，同样的微服务，有 10 个节点是给移动用户服务的，有 20 个节点给联通用户服务的，配置项都一样，配置值不一样）、增删改查配置、节点管理、配置同步、配置推送等功能 接口框架 接口框架不是一个可运行的系统，一般以库或者包的形式提供给所有微服务调用微服务提倡轻量级的通信方式，一般采用 HTTP/REST 或者 RPC 方式统一接口协议。但在实践过程中，光统一接口协议还不够，还需要统一接口传递的数据格式(例如: 要指定 HTTP/REST 的数据格式采用 JSON，并且 JSON 的数据都遵循一定规范) API网关 为什么需要API网关 在外部系统看来，它不需要也没办法理解众多微服务的职责分工和边界，它只会关注它需要的能力，而不会关注这个能力应该由哪个微服务提供。 除此以外，外部系统访问系统还涉及安全和权限相关的限制，如果外部系统直接访问某个微服务，则意味着每个微服务都要自己实现安全和权限的功能，这样做不但工作量大，而且都是重复工作。 综合上面的分析，微服务需要一个统一的 API 网关，负责外部系统的访问操作。 API 网关是外部系统访问的接口，所有的外部系统接入系统都需要通过API网关，主要包括接入鉴权（是否允许接入）、权限控制（可以访问哪些功能）、传输加密、请求路由、流量控制等功能。 搭建微服务基础设施 按照一定优先级搭建微服务基础设施 服务发现、服务路由、服务容错 这是最基本的微服务基础设施 接口框架、API网关 主要为了提升开发效率，接口框架是提升内部服务的开放效率，API网管是为了提升与外部服务对接的效率 自动化部署、自动化测试、配置中心 主要为了提升测试和运维效率 服务监控、服务跟踪、服务安全 主要为了进一步提升运维效率 第三点和第四点两类基础设施，其重要性会随着微服务节点数量增加而越来越重要，但在微服务节点数量较少的时候，可以通过人工的方式支撑。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/he/microkernel.html":{"url":"docs/he/microkernel.html","title":"微内核架构","keywords":"","body":"微内核架构基本架构设计关键点OSGi 架构简析规则引擎架构简析微内核架构 微内核架构（Microkernel Architecture），也被称为插件化架构（Plug-in Architecture），是一种面向功能进行拆分的可扩展性架构，通常用于实现基于产品的应用例如: Eclipse 这类 IDE 软件、UNIX 这类操作系统、淘宝 App 这类客户端软件等 基本架构 微内核架构包含两类组件：核心系统（core system）和插件模块（plug-in modules）核心系统负责和具体业务功能无关的通用功能，例如模块加载、模块间通信等;插件模块负责实现具体的业务逻辑 核心系统 Core System 功能比较稳定，不会因为业务功能扩展而不断修改，插件模块可以根据业务功能的需要不断地扩展。微内核的架构本质就是将变化部分封装在插件里面，从而达到快速灵活扩展的目的，而又不影响整体系统的稳定 设计关键点 插件管理 核心系统需要知道当前有哪些插件可用，如何加载这些插件，什么时候加载插件。 常见的实现方法是插件注册表机制。 核心系统提供插件注册表（可以是配置文件，也可以是代码，还可以是数据库），插件注册表含有每个插件模块的信息，包括它的名字、位置、加载时机（启动就加载，还是按需加载）等。 插件连接 插件连接指插件如何连接到核心系统。 通常来说，核心系统必须制定插件和核心系统的连接规范，然后插件按照规范实现，核心系统按照规范加载即可。 常见的连接机制有 OSGi（Eclipse 使用）、消息模式、依赖注入（Spring 使用），甚至使用分布式的协议都是可以的，比如 RPC 或者 HTTP Web 的方式。 插件通信 插件通信指插件间的通信。 由于插件之间没有直接联系，通信必须通过核心系统，因此核心系统需要提供插件通信机制。 OSGi 架构简析 OSGi 的全称是 Open Services Gateway initiative，本身其实是指 OSGi Alliance。一个开放的服务规范，为通过网络向设备提供服务建立开放的标准 模块层(Moudle层) 模块层实现插件管理功能。 OSGi 中，插件被称为 Bundle，每个Bundle里面都包含一个元数据文件MANIFEST.MF，这个文件包含了 Bundle 的基本信息。例如，Bundle 的名称、描述、开发商、classpath，以及需要导入的包和输出的包等，OSGi 核心系统会将这些信息加载到系统中用于后续使用 MANIFEST.MF样例 // MANIFEST.MF Bundle-ManifestVersion: 2 Bundle-Name:UserRegister Bundle-SymbolicName: com.test.userregister Bundle-Version: 1.0 Bundle-Activator: com.test.UserRegisterActivator Import-Package: org.log4j;version=\"2.0\", ..... Export-Package: com.test.userregister;version=\"1.0\", 生命周期层(Lifeycle层) 生命周期层实现插件连接功能，提供了执行时模块管理、模块对底层OSGi框架的访问。 生命周期层精确地定义了Bundle生命周期的操作(安装、更新、启动、停止、卸载)， Bundle 必须按照规范实现各个操作 服务层(Service层) 服务层实现插件通信的功能。 OSGi提供了一个服务注册的功能，用于各个插件将自己能提供的服务注册到OSGi核心的服务注册中心，如果某个服务想用其他服务，则直接在服务注册中心搜索可用服务中心就可以了 这里的服务注册不是插件管理功能中的插件注册，实际上是插件间通信的机制 规则引擎架构简析 规则引擎从结构上来看也属于微内核架构的一种具体实现，其中执行引擎可以看作是微内核，执行引擎解析配置好的业务流，执行其中的条件和规则，通过这种方式来支持业务的灵活多变。 可扩展 业务逻辑实现与业务系统分离，可以在不改动业务系统的情况下扩展新的业务功能。 易理解 规则通过自然语言描述，业务人员易于理解和操作，而不像代码那样只有程序员才能理解和开发 高效率 规则引擎系统一般提供可视化的规则定制、审批、查询及管理，方便业务人员快速配置新的业务 开发人员将业务功能分解提炼为多个规则，将规则保存在规则库中。 业务人员根据业务需要，通过将规则排列组合，配置成业务流程，保存在业务库中。 规则引擎执行业务流程实现业务功能。 规则引擎对微内核架构的设计关键点实现 插件管理 规则引擎中的规则就是微内核架构的插件，引擎就是微内核架构的内核。 规则可以被引擎加载和执行。规则引擎架构中，规则一般保存在规则库中，通常使用数据库来存储。 插件链接 规则引擎规定了规则开发的语言，业务人员需要基于规则语言来编写规则文件，然后由规则引擎加载执行规则文件来完成业务功能，因此，规则引擎的插件连接实现机制其实就是规则语言 插件通信 规则引擎的规则之间进行通信的方式就是数据流和事件流，由于单个规则并不需要依赖其他规则，因此规则之间没有主动的通信，规则只需要输出数据或者事件，由引擎将数据或者事件传递到下一个规则。 有的规则语言可能比较类似于代码, 需要再次封装成可视化的操作. Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/combat/direction.html":{"url":"docs/combat/direction.html","title":"技术演进的方向","keywords":"","body":"技术演进的方向技术演进的动力技术演进的模式技术演进的方向 技术演进的动力 市场、技术、管理，这三者构成支撑业务发展的铁三角，任何一个因素的不足，都可能导致企业的业务停滞不前 业务分类 产品类 技术创新推动业务发展 360 的杀毒软件、苹果的 iPhone、UC 的浏览器等都属于这个范畴 服务类 业务发展推动技术的发展 百度的搜索、淘宝的购物、新浪的微博、腾讯的IM 等都属于这个范畴 大量用户使用这些服务来完成需要与其他人交互的任务，单个用户“使用”但不“独占”某个服务。事实上，服务的用户越多，服务的价值就越大。服务类的业务符合互联网的特征和本质：“互联”+“网”。 技术演进的模式 基于业务发展阶段进行判断技术演进的模式 架构师必须具备业务理解能力 不同的行业业务发展路径、轨迹、模式不一样，架构师必须能够基于行业发展和企业自身情况做出准确判断 互联网业务发展的时期 初创期 发展期 竞争期 成熟期 不同时期的差别主要体现在：复杂性和用户规模 业务复杂性 初创期 互联网业务刚开始一般都是一个创新的业务点，这个业务点重点不在于“完善”，而在于“创新”，只有创新才能吸引用户。 初创期的业务对技术的要求：“快” 这个时期是团队最弱小的时期，可能就几个技术人员，所以这时候是能买就买，有开源就用开源。 发展期 当用户越来越多后，原来不完善的业务就进入了一个快速发展的时期。 业务快速发展时期的主要目的是将原来不完善的业务逐渐完善，因此会有越来越多的新功能不断加入到系统中。 这个阶段技术的核心工作室快速实现各种需求。 发展期的几个阶段 堆功能阶段 最快实现业务需求的方式是在原有的系统里面不断的增加新的功能。 优化阶段 当功能越来越多时，系统就越来越复杂，再堆功能会感到很吃力，很慢。 这时采用系统优化能够很快解决当前问题（重构、分层、组件更换、硬件更换等） 架构阶段 当经过优化后的系统也无法支撑业务的发展时，就需要进行架构调整了。 架构期的手段就是“拆”（可见前面笔记记录） 竞争期 当有竞争对手加入后，大家互相学习和模仿，业务更加完善，也不断有新的业务创新出来，这会导致系统会更多，从而引起质变。 体现 重复造轮子 系统越来越多，各系统相似的功能也越来越多。 系统交互乱 系统越来越多，各系统的交互关系变成了网状。解决手段 平台化 解决重复造轮子问题 存储平台化：淘宝tfs、京东jfs 数据库平台化：百度dbproxy、淘宝tddl。 缓存平台化：twitter的twemproxy、豆瓣的beansdb、腾讯的ttc。 服务化 解决系统交互问题 消息队列：淘宝notify、metaq、开源的kafka、activemq等。 服务框架：Facebook的thrift、当当网的dubbox、淘宝的hsf等。 成熟期 这时候成为了行业的领头羊，或者整个行业整体上已经处于比较成熟的阶段。 此时业务创新很少，业务上倾向于“求精”（响应更快、用户体验更好） 用户规模随着业务发展的几个阶段，用户量也会越来越大。用户量增加对技术影响：性能要求越来越高、可用性要求越来越高。 性能 用户量增加对性能要求提升。 可用性 用户量更大时，如果出现宕机会造成不可估量的损失。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/combat/intertemplate/":{"url":"docs/combat/intertemplate/","title":"互联网架构模板","keywords":"","body":"互联网架构模板互联网架构模板 互联网标准技术架构图 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/combat/intertemplate/store.html":{"url":"docs/combat/intertemplate/store.html","title":"存储层","keywords":"","body":"存储层SQLNoSQL存储平台主要功能存储层 SQL 中间件 使用中间件统一业务调用层 基于sql集群上构建sql存储平台 如淘宝的UMP(Unified MySQL Platform)系统 规模更大时，使用sql存储平台，充分利用数据库使用率。 NoSQL NoSQL一般都有自己本身的集群功能(不像SQL那样分库分表那么复杂)当发展到一定规模后，在NoSQL集群基础上再实现统一存储平台。 存储平台主要功能 资源按需动态分配 例如同一台memcache服务器，可以根据内存使用率，分配给多个业务使用 资源自动化管理 例如新业务只需要多少memcache缓存空间就可以了，无须关注具体是哪些memcache服务器在为自己提供服务。 故障自动化处理 例如某台memcache服务器挂掉，有另外一台备份memcache服务器能够立即接管缓存请求。 小文件存储 互联网行业基本每个业务都会有大量的小数据，如果每个业务都自己考虑如何设计存储和海量访问，效率会很低，所以小文件存储做成统一的和业务无关的平台是有必要的。 淘宝TFS架构 大文件存储 业务上的大数据 视频、电影等 海量的日志数据 Hadoop生态圈 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/combat/intertemplate/develop.html":{"url":"docs/combat/intertemplate/develop.html","title":"开发层","keywords":"","body":"开发层开发框架Web服务器容器开发层 开发框架 如果每个小组用不同的开发框架和技术，会带来很多问题 技术人员之间没有共同的技术语言，交流合作少 每类技术都需要投入大量的人力和资源并熟练精通 不同团队之间人员无法快速流动，人力资源不能高效利用 框架选择：优选成熟的框架，避免盲目追逐新技术！ 成熟框架资料文档齐备，问题很容易解决 更容易招到合适的人才 更加稳定，不会出现大的变动 Web服务器 选择服务主要和开发语言相关如java的tomcat、jboss、resin等，php/python用nginx，或者apache支持多种语言。 容器 容器很大程度上改变了技术形势 运维方式会发生革命性的变化 docker启动快，几乎不占资源，随时启动和停止，基于docker打造自动化运维、智能化运维成为主流方式 设计模式会发生本质上的变化 启动一个新的容器实例代价低，将鼓励设计思路朝\"微服务\"的方向发展 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/combat/intertemplate/service.html":{"url":"docs/combat/intertemplate/service.html","title":"服务层","keywords":"","body":"服务层配置中心服务中心消息队列服务层 服务层的主要目标是为了降低系统间的相互关联的复杂度 配置中心 集中管理各个系统的配置 特点 集中配置多个系统，操作效率高 所有配置在一个集中的地方，检查方便，写作效率高 配置中心可以实现程序化的规则检查，避免常见错误 比如检查最小值、最大值、是否是ip地址、是否是url地址等(都可以用正则表达式完成) 配置中心相当于备份了系统的配置，当某些情况下需要搭建新的环境时，能够快速搭建环境和恢复业务 配置中心简单设计 服务中心 系统间的调用一般通过配置文件记录在各个系统内部，当某个系统接口发生变更时，需要修改大量的配置文件其他系统通过ip访问a系统，如果a系统每次增加或者删除机器，其他所有系统都要同步修改，工作量很大。服务中心为了解决跨系统依赖的“配置”和“调度”问题发生时，避免少量数据异常导致整体业务不可用。 服务中心实现 服务名字系统 与dns相似，服务名字系统将service名称解析为“host + port + 接口名称” 基本设计如下 服务总线系统由总线系统完成调用，服务请求方不需要直接和服务提供方交互基本设计如下 消息队列 传统的异步通知方式是由消息生产者直接调用消息消费者提供的接口进行通知的，当业务子系统数量增多时，会导致系统间交互非常复杂和难以管理（因为系统间相互依赖和调用），会形成一个网状的模式。消息队列实现了跨系统异步通知的中间件系统。 优点 整体结构从网状结构变为线性接口，结构清晰 消息生产和消息消费解耦，实现简单 增加新的消费者，消息生产者完全不需要改动，扩展方便 消息队列可以做到高可用、高性能、避免各业务子系统各自独立做一套，减轻工作量 业务子系统只需要聚焦业务即可，实现简单 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/combat/intertemplate/internet.html":{"url":"docs/combat/intertemplate/internet.html","title":"网络层","keywords":"","body":"网络层负载均衡CDN多机房多中心网络层 负载均衡 DNS Nginx、LVS、F5 CDN 解决用户网络访问时的“最后一公里”效应，本质上是“以空间换时间”的加速策略，既将内容缓存在离用户最近的地方，用户访问的是缓存的内容，而不是站点实时的内容。 多机房 同城多机房 跨城多机房 跨国多机房 多中心 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/combat/intertemplate/user.html":{"url":"docs/combat/intertemplate/user.html","title":"用户层","keywords":"","body":"用户层用户管理消息推送存储云、图片云用户层 用户管理 互联网业务的一个典型特征是通过互联网将众多分散的用户连接起来。 单点登录（SSO）（统一登录）单点登录成熟开源方案CAS架构如下 授权登录业务做成平台后，需要允许第三方应用接入，这就需要授权登录。流行协议：OAuth 2.0协议 用户管理基本架构 消息推送 短信、邮件、站内信、App推送。一般都有对应的第三方接入api。 对于敏感数据，可能需要自己实现消息推送 实现消息推送 海量设备和用户管理 将用户和设备关联起来，需要提取用户特征对用户进行分类或者打标签等。 连接保活 要想推送消息必须有连接通道，但大部分设备为了省电省流量等原因会限制应用后台运行，这可能会导致连接通道被中断，导致消息无法及时送达。 连接保活是整个消息推送里面设计中细节和黑科技最多的地方（例如应用互相拉起、找手机厂商开白名单等）。 消息管理 不是每个消息都需要发送给每个用户，而是根据用户的特征，宣和一些用户进行消息推送。 可以采用规则引擎之类的微内核架构的技术。 存储云、图片云 互联网业务场景中，用户会上传多种类型的文件数据（如微信朋友圈图片，微博视频图片等） 特点 数量大：用户基数大，用户上传频繁 文件体积小：大部分是几百KB到几MB 访问有效性：大部分是刚上传的时候访问最多，随着时间的推移访问量越来越小 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/combat/intertemplate/business.html":{"url":"docs/combat/intertemplate/business.html","title":"业务层","keywords":"","body":"业务层业务层 业务层的主要技术挑战是：“复杂度” 简单电商系统 第一阶段：所有功能都在一个系统里面 第二阶段：将商品和订单拆分为两个子系统里面 第三阶段：商品子系统和订单子系统分别拆分为更小的多个子系统 随着子系统数量越来越多，如果达到几百上千，有会出现新的复杂度问题： 子系统数量太多，已经没有人可以说清楚业务的调用流程，出了问题排查特别复杂。 解决方法按照“高内聚、低耦合”原则进行“合”，将职责关联性比较强的子系统合成一个虚拟业务域，然后通过网关对外统一呈现。 虚拟业务域架构 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/combat/intertemplate/platform.html":{"url":"docs/combat/intertemplate/platform.html","title":"平台","keywords":"","body":"平台运维系统运维平台核心的职责运维平台的核心设计要素测试平台数据平台管理平台平台 运维系统 运维平台核心的职责 配置 主要负责资源管理（机器管理、ip地址管理、虚拟机管理等） 部署 主要负责将系统发不到线上（包管理、灰度发布管理、回滚等） 监控 主要负责收集系统上线后的相关数据并进行监控 应急 主要负责系统出故障后的处理（停止程序、下线故障机、切换ip等） 运维平台的核心设计要素 标准化 需要制定运维标准，规范配置管理、部署流程、监控指标、应急能力等，各系统按照运维标准来实现，避免不同的系统不同的处理方式。 标准化是运维平台的基础，没有标准化就没有运维平台。 平台化 将运维的相关操作都集成到运维平台中，通过运维平台来完成运维工作。 优点 可以将运维标准固化到平台中，无须运维人员死记硬背运维标准 运维平台提供简单的操作（相比之下人工操作低效且容易出错） 运维平台是可复用的，一套运维平台可以支撑几百上千个业务系统 自动化 运维平台将重复操作固化下来，由系统自动完成。 可视化 可视化的主要目的是为了提升数据查看的效率。 优点 能够直观的看到数据的相关属性 能够将数据的含义展示出来 能够将关联数据整合一起展示 测试平台 单元测试、集成测试、接口测试、性能测试等都是测试平台的职责。测试平台的核心目的是提升测试效率，从而提升产品质量，设计关键就是自动化。 测试平台基本架构 用例管理 测试平台将用例管理起来，能够重复执行测试步骤。 管理的纬度包括业务、系统、测试类型、用例代码。 资源管理 测试用例要放到具体的运行环境才能真正执行，运行环境包括硬件（服务器、手机、平板等）、软件（操作系统、数据库、java虚拟机等）、业务系统（被测试系统）。 除了性能测试，一般自动化测试对性能要求不高，所以大部分测试平台都会使用虚拟技术来充分利用硬件资源（如虚拟机、docker等） 任务管理 任务管理的主要职责是将测试用例分配到具体的资源上执行，跟踪任务的执行情况。 任务管理是测试平台设计的核心，它将测试平台的各个部分串联起来从而完成自动化测试。 数据管理 测试任务执行完成后，需要记录各种相关的数据（如执行时间、执行结果、执行期间cpu、内存占用等） 这些数据的作用 展现当前用例的执行情况 作为历史数据，方便后续的测试与历史数据进行比对 作为大数据的一部分，可以基于测试的任务数据进行一些数据挖掘 数据平台 数据平台架构 数据管理 数据采集：从业务系统手机各类数据（如日志、用户行为、业务数据等），将这些数据传送到数据平台 数据存储：将从业务系统采集的数据存储到数据平台，用于后续数据分析 数据访问：负责对外提供各种协议用于读写数据（如sql、hive、key－value等读写协议） 数据安全：通常数据平台多个业务恭喜数据，部分业务敏感数据需要保护，房子被其他业务读取甚至修改 数据分析 数据统计：根据原始数据统计出相关的总览数据（如pv、uv、交易额等） 数据挖掘：数据分析人员基于数据仓库构建一系列规则来对数据进行分析从而发现一些隐含的规律、现象、问题等（经典的数据挖掘案例是沃尔玛的啤酒和尿布的关联关系发现）。 机器学习、深度学习：机器学习和深度学习属于数据挖掘的一种具体实现方式，由于其实现方式与传统的数据挖掘方式差异较大，因此数据平台在实现机器学习和深度学习时，需要独立进行设计。 数据应用 数据应用很广泛，包括在线业务，也包括离线业务（如推荐、广告当等属于在线用用，报表、欺诈检测、异常检测等数据离线应用）。 数据应用能够发挥价值的前提是需要有“大数据”，只有当数据的规模到达一定程度，基于数据的分析、挖掘才能发现有价值的规律、现象、问题等。 管理平台 管理平台的核心职责是权限管理 权限管理基本架构 身份认证 确认当前的操作人员身份，防止非法人员进入系统。 权限控制 根据操作人员的身份确定操作权限，防止未经授权的人员进行操作。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/combat/refactor.html":{"url":"docs/combat/refactor.html","title":"架构重构","keywords":"","body":"架构重构有的放矢重构案例判断架构是否需要重构的简单方法合纵连横合纵连横运筹帷幄架构重构 架构重构难点 业务已经上线，不能停下来 关联方众多，牵一发动全身 架构重构涉及的业务关联方很多，不同关联方的资源投入程度、业务发展速度、对架构痛点的敏感度等有很大差异 旧架构的约束 架构重构需要在旧的架构基础上进行，这是一个很强的约束，会限制架构师的技术选择范围 有的放矢 从一大堆纷繁复杂的问题中识别出真正要通过架构重构来解决的问题，集中力量快速解决，而不是想着通过架构重构来解决所有的问题 重构案例 后台系统重构：解决不合理的耦合 M 系统是一个后台管理系统，负责管理所有游戏相关的数据，重构的主要原因是因为系统耦合了 P 业务独有的数据和所有业务公用的数据，导致可扩展性比较差。 基本架构 问题例子: 数据库中的某张表，一部分字段是所有业务公用的“游戏数据”，一部分字段是 P 业务系统“独有的数据”，开发时如果要改这张表，代码和逻辑都很复杂，改起来效率很低 针对 M 系统存在的问题，重构目标就是将游戏数据和业务数据拆分，解开两者的耦合，使得两个系统都能够独立快速发展 重构后架构 判断架构是否需要重构的简单方法 假设我们现在需要从 0 开始设计当前系统，新架构和老架构是否类似？如果差异不大，说明采取系统优化即可；如果差异很大，那可能就要进行系统重构了。 合纵连横 合纵 架构重构是大动作，持续时间比较长，而且会占用一定的研发资源，包括开发和测试，因此不可避免地会影响业务功能的开发。因此，要想真正推动一个架构重构项目启动，需要花费大量的精力进行游说和沟通,让大家对于重构能够达成一致共识，避免重构过程中不必要的反复和争执。 在沟通协调时，将技术语言转换为通俗语言，以事实说话，以数据说话，是沟通的关键！ 连横 有的重构还需要和其他相关或者配合的系统的沟通协调。由于大家都是做技术的，有比较多的共同语言，所以这部分的沟通协调其实相对来说要容易一些，但也不是说想推动就能推动的，主要的阻力来自“这对我有什么好处”和“这部分我这边现在不急”。 推动策略换位思考、合作双赢、关注长期简单来说就是站在对方的角度思考，重构对他有什么好处，能够帮他解决什么问题，带来什么收益如果真的出现了对公司或者部门有利，对某个小组不利的情况，那可能需要协调更高层级的管理者才能够推动，平级推动是比较难的如果对方真的有其他更加重要的事情，采取等待的策略也未尝不可，但要明确正式启动的时间。例如，3 个月后开始、6 月份开始，千万不能说“以后”“等不忙的时候”这种无法明确的时间点。 方案上灵活运用可以先不做这个系统相关的重构，先把其他需要重构的做完。因为大部分需要重构的系统，需要做的事情很多，分阶段处理，在风险规避、计划安排等方面更加灵活可控 运筹帷幄 将要解决的问题根据优先级、重要性、实施难度等划分为不同的阶段，每个阶段聚焦于一个整体的目标，集中精力和资源解决一类问题。 每个阶段都有明确目标，做完之后效果明显，团队信心足，后续推进更加容易。 每个阶段的工作量不会太大，可以和业务并行。 每个阶段的改动不会太大，降低了总体风险。 具体做法 优先级排序 将明显且又比较紧急的事项优先落地，解决目前遇到的主要问题。 问题分类 将问题按照性质分类，每个阶段集中解决一类问题。 先易后难 循序渐进 要评估每个阶段所需要耗费的时间，通常情况下，按照固定的步骤和节奏，更有利于项目推进。 一般每个阶段最少 1 个月，最长不要超过 3 个月，如果评估超过 3 个月的，那就再拆分为更多阶段 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/combat/opensource.html":{"url":"docs/combat/opensource.html","title":"开源项目","keywords":"","body":"开源项目如何高效学习开源项目如何学习开源项目自顶向下步骤如何选择、使用以及二次开发开源项目如何选择如何使用如何二次开发*开源项目 如何高效学习开源项目 如何学习开源项目 需要树立正确的观念 不管你是什么身份，都可以从开源项目中学到很多东西 不要只盯着数据结构和算法 事实上这两点在学习开源项目的时候并没有那么重要 采取“自顶向下”的学习方法，源码不是第一步，而是最后一步 自顶向下步骤 安装 运行 命令和配置文件 如果能够将每个配置项的作用和原理都掌握清楚,基本上对系统已经很熟悉了. 原理研究 针对原理进行系统性研究 关键特性的基本实现原理 优缺点对比分析 只有清楚掌握技术方案的优缺点后才算真正的掌握这门技术，也只有掌握了技术方案的优缺点后才能在架构设计的时候做出合理的选择 将两个类似的系统进行对比,找出优缺点.原理研究的手段 通读项目的设计文档 阅读网上已有的分析文档 demo验证 如果有些技术点难以查到资料，自己又不确定，则可以真正去写 Demo 进行验证，通过打印一些日志或者调试，能清晰的理解具体的细节 测试 通常情况下，如果你真的准备在实际项目中使用某个开源项目的话，必须进行测试 源码研究 不建议通读所有源码 带着明确目的去研究源码，做到有的放矢，才能事半功倍 如何选择、使用以及二次开发开源项目 如何选择 聚焦是否满足业务 聚焦于是否满足业务，而不需要过于关注开源项目是否优秀。 聚焦是否成熟 版本号 使用的公司数量 一般开源项目都会把采用了自己项目的公司列在主页上，公司越大越好，数量越多越好 社区活跃度 聚焦运维能力 开源项目日志是否齐全 开源项目是否有命令行、管理控制台等维护工具，能够看到系统运行时的情况 开源项目是否有故障检测和恢复的能力，例如告警、切换等 如何使用 深入研究，仔细测试 小心应用，灰度发布 做好应急，以防万一 如何二次开发* 保持纯洁，加以包装 不要改动原系统，而是要开发辅助系统：监控、报警、负载均衡、管理等 发明你要的轮子 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/template/options.html":{"url":"docs/template/options.html","title":"备选方案模板","keywords":"","body":"备选方案模板1. 需求介绍2. 需求分析3. 复杂度分析4. 备选方案5. 备选方案评估备选方案模板 1. 需求介绍 [需求介绍主要描述需求的背景、目标、范围等] 随着前浪微博业务的不断发展，业务上拆分的子系统越来越多，目前系统间的调用都是同步调用，由此带来几个明显的系统问题： 性能问题：当用户发布了一条微博后，微博发布子系统需要同步调用“统计子系统”“审核子系统”“奖励子系统”等共 8 个子系统，性能很低。 耦合问题：当新增一个子系统时，例如如果要增加“广告子系统”，那么广告子系统需要开发新的接口给微博发布子系统调用。 效率问题：每个子系统提供的接口参数和实现都有一些细微的差别，导致每次都需要重新设计接口和联调接口，开发团队和测试团队花费了许多重复工作量。 基于以上背景，我们需要引入消息队列进行系统解耦，将目前的同步调用改为异步通知。 2. 需求分析 [需求分析主要全方位地描述需求相关的信息] 5W[5W 指 Who、When、What、Why、Where Who：需求利益干系人，包括开发者、使用者、购买者、决策者等。 When：需求使用时间，包括季节、时间、里程碑等。 What：需求的产出是什么，包括系统、数据、文件、开发库、平台等。 Where：需求的应用场景，包括国家、地点、环境等，例如测试平台只会在测试环境使用。 Why：需求需要解决的问题，通常和需求背景相关] 消息队列的 5W 分析如下 Who：消息队列系统主要是业务子系统来使用，子系统发送消息或者接收消息。 When：当子系统需要发送异步通知的时候，需要使用消息队列系统。 What：需要开发消息队列系统。 Where：开发环境、测试环境、生产环境都需要部署。 Why：消息队列系统将子系统解耦，将同步调用改为异步通知。 1H [这里的 How 不是设计方案也不是架构方案，而是关键业务流程。消息队列系统这部分内容很简单，但有的业务系统 1H 就是具体的用例了，有兴趣的同学可以尝试写写 ATM 机取款的业务流程。如果是复杂的业务系统，这部分也可以独立成“用例文档”] 消息队列有两大核心功能： 业务子系统发送消息给消息队列。 业务子系统从消息队列获取消息。 8C [8C 指的是 8 个约束和限制，即 Constraints，包括性能 Performance、成本 Cost、时间 Time、可靠性 Reliability、安全性 Security、合规性 Compliance、技术性 Technology、兼容性 Compatibility] 注：需求中涉及的性能、成本、可靠性等仅仅是利益关联方提出的诉求，不一定准确；如果经过分析有的约束没有必要，或成本太高、难度太大，这些约束是可以调整的。 性能：需要达到 Kafka 的性能水平。 成本：参考 XX 公司的设计方案，不超过 10 台服务器。 时间：期望 3 个月内上线第一个版本，在两个业务尝试使用。 可靠性：按照业务的要求，消息队列系统的可靠性需要达到 99.99%。 安全性：消息队列系统仅在生产环境内网使用，无需考虑网络安全；如消息中有敏感信息，消息发送方需要自行进行加密，消息队列系统本身不考虑通用的加密。 合规性：消息队列系统需要按照公司目前的 DevOps 规范进行开发。 技术性：目前团队主要研发人员是 Java，最好用 Java 开发。 兼容性：之前没有类似系统，无需考虑兼容性。 3. 复杂度分析 [分析需求的复杂度，复杂度常见的有高可用、高性能、可扩展等，具体分析方法请参考专栏前面的内容] 注：文档的内容省略了分析过程，实际操作的时候每个约束和限制都要有详细的逻辑推导，避免完全拍脑袋式决策 高可用 对于微博子系统来说，如果消息丢了，导致没有审核，然后触犯了国家法律法规，则是非常严重的事情；对于等级子系统来说，如果用户达到相应等级后，系统没有给他奖品和专属服务，则 VIP 用户会很不满意，导致用户流失从而损失收入，虽然也比较关键，但没有审核子系统丢消息那么严重。 综合来看，消息队列需要高可用性，包括消息写入、消息存储、消息读取都需要保证高可用性。 高性能 前浪微博系统用户每天发送 1000 万条微博，那么微博子系统一天会产生 1000 万条消息，平均一条消息有 10 个子系统读取，那么其他子系统读取的消息大约是 1 亿次。将数据按照秒来计算，一天内平均每秒写入消息数为 115 条，每秒读取的消息数是 1150 条；再考虑系统的读写并不是完全平均的，设计的目标应该以峰值来计算。峰值一般取平均值的 3 倍，那么消息队列系统的 TPS 是 345，QPS 是 3450，考虑一定的性能余量。由于现在的基数较低，为了预留一定的系统容量应对后续业务的发展，我们将设计目标设定为峰值的 4 倍，因此最终的性能要求是：TPS 为 1380，QPS 为 13800。TPS 为 1380 并不高，但 QPS 为 13800 已经比较高了，因此高性能读取是复杂度之一。 可扩展消息队列的功能很明确，基本无须扩展，因此可扩展性不是这个消息队列的关键复杂度。 4. 备选方案 [备选方案设计，至少 3 个备选方案，每个备选方案需要描述关键的实现，无须描述具体的实现细节。此处省略具体方案描述] 备选方案 1：直接引入开源 Kafka [此处省略方案描述] 备选方案 2：集群 + MySQL 存储 [此处省略方案描述] 备选方案 3：集群 + 自研存储 [此处省略方案描述] 5. 备选方案评估 [备选方案 360 度环评，注意备选方案评估的内容会根据评估会议的结果进行修改，也就是说架构师首先给出自己的备选方案评估，然后举行备选方案评估会议，再根据会议结论修改备选方案文档] Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/template/architectDesign.html":{"url":"docs/template/architectDesign.html","title":"架构设计模板","keywords":"","body":"架构设计模板1. 总体方案2. 架构总览3. 核心流程4. 详细设计5. 架构演进规划架构设计模板 [备选方案评估后会选择一个方案落地实施，架构设计文档就是用来详细描述细化方案的] 1. 总体方案 [总体方案需要从整体上描述方案的结构，其核心内容就是架构图，以及针对架构图的描述，包括模块或者子系统的职责描述、核心流程] 2. 架构总览 [架构总览给出架构图以及架构的描述] 架构关键设计点: 采用数据分散集群的架构，集群中的服务器进行分组，每个分组存储一部分消息数据。 每个分组包含一台主 MySQL 和一台备 MySQL，分组内主备数据复制，分组间数据不同步。 正常情况下，分组内的主服务器对外提供消息写入和消息读取服务，备服务器不对外提供服务；主服务器宕机的情况下，备服务器对外提供消息读取的服务。 客户端采取轮询的策略写入和读取消息。 3. 核心流程 消息发送流程 [此处省略流程描述] 消息读取流程 [此处省略流程描述] 4. 详细设计 [详细设计需要描述具体的实现细节] 高可用设计 消息发送可靠性 业务服务器中嵌入消息队列系统提供的 SDK，SDK 支持轮询发送消息，当某个分组的主服务器无法发送消息时，SDK 挑选下一个分组主服务器重发消息，依次尝试所有主服务器直到发送成功；如果全部主服务器都无法发送，SDK 可以缓存消息，也可以直接丢弃消息，具体策略可以在启动 SDK 的时候通过配置指定。 如果 SDK 缓存了一些消息未发送，此时恰好业务服务器又重启，则所有缓存的消息将永久丢失，这种情况 SDK 不做处理，业务方需要针对某些非常关键的消息自己实现永久存储的功能。 消息存储可靠性 消息存储在 MySQL 中，每个分组有一主一备两台 MySQL 服务器，MySQL 服务器之间复制消息以保证消息存储高可用。如果主备间出现复制延迟，恰好此时 MySQL 主服务器宕机导致数据无法恢复，则部分消息会永久丢5失，这种情况不做针对性设计，DBA 需要对主备间的复制延迟进行监控，当复制延迟超过 30 秒的时候需要及时告警并进行处理。 消息读取可靠性 每个分组有一主一备两台服务器，主服务器支持发送和读取消息，备服务器只支持读取消息，当主服务器正常的时候备服务器不对外提供服务，只有备服务器判断主服务器故障的时候才对外提供消息读取服务。 主备服务器的角色和分组信息通过配置指定，通过 ZooKeeper 进行状态判断和决策。主备服务器启动的时候分别连接到 ZooKeeper，在 /MQ/Server/[group]目录下建立 EPHEMERAL 节点，假设分组名称为 group1，则主服务器节点为 /MQ/Server/group1/master，备服务器的节点为 /MQ/Server/group1/slave。节点的超时时间可以配置，默认为 10 秒。 高性能设计 [此处省略具体设计] 可扩展设计 [此处省略具体设计。如果方案不涉及，可以简单写上“无”，表示设计者有考虑但不需要设计；否则如果完全不写的话，方案评审的时候可能会被认为是遗漏了设计点] 无 安全设计 消息队列系统需要提供权限控制功能，权限控制包括两部分：身份识别和队列权限控制。 身份识别 消息队列系统给业务子系统分配身份标识和接入 key，SDK 首先需要建立连接并进行身份校验，消息队列服务器会中断校验不通过的连接。因此，任何业务子系统如果想接入消息队列系统，都必须首先申请身份标识和接入 key，通过这种方式来防止恶意系统任意接入。 队列权限 某些队列信息可能比较敏感，只允许部分子系统发送或者读取，消息队列系统将队列权限保存在配置文件中，当收到发送或者读取消息的请求时，首先需要根据业务子系统的身份标识以及配置的权限信息来判断业务子系统是否有权限，如果没有权限则拒绝服务。 其他设计 [其他设计包括上述以外的其他设计考虑点，例如指定开发语言、符合公司的某些标准等，如果篇幅较长，也可以独立进行描述] 消息队列系统需要接入公司已有的运维平台，通过运维平台发布和部署。 消息队列系统需要输出日志给公司已有的监控平台，通过监控平台监控消息队列系统的健康状态，包括发送消息的数量、发送消息的大小、积压消息的数量等，详细监控指标在后续设计方案中列出。 部署方案 [部署方案主要包括硬件要求、服务器部署方式、组网方式等] 消息队列系统的服务器和数据库服务器采取混布的方式部署，即：一台服务器上，部署同一分组的主服务器和主 MySQL，或者备服务器和备 MySQL。因为消息队列服务器主要是 CPU 密集型，而 MySQL 是磁盘密集型的，所以两者混布互相影响的几率不大。 硬件的基本要求：32 核 48G 内存 512G SSD 硬盘，考虑到消息队列系统动态扩容的需求不高，且对性能要求较高，因此需要使用物理服务器，不采用虚拟机。 5. 架构演进规划 [通常情况下，规划和设计的需求比较完善，但如果一次性全部做完，项目周期可能会很长，因此可以采取分阶段实施，即：第一期做什么、第二期做什么，以此类推] 整个消息队列系统分三期实现： 第一期：实现消息发送、权限控制功能，预计时间 3 个月。 第二期：实现消息读取功能，预计时间 1 个月。 第三期：实现主备基于 ZooKeeper 切换的功能，预计时间 2 周。 Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "},"docs/other/kafka_rocketmq.html":{"url":"docs/other/kafka_rocketmq.html","title":"kafka和RocketMQ区别","keywords":"","body":" 数据可靠性kafka使用异步刷盘方式，异步ReplicationRocketMQ支持异步刷盘，同步刷盘，同步Replication，异步Replication 严格的消息顺序 Kafka支持消息顺序，但是一台Broker宕机后，就会产生消息乱序 RocketMQ支持严格的消息顺序，在顺序消息场景下，一台Broker宕机后，发送消息会失败，但是不会乱序 消费失败重试机制 Kafka消费失败不支持重试 RocketMQ消费失败支持定时重试，每次重试间隔时间顺延 定时消息Kafka不支持定时消息RocketMQ支持定时消息 分布式事务消息Kafka不支持分布式事务消息阿里云ONS支持分布式定时消息，未来开源版本的RocketMQ也有计划支持分布式事务消息 消息查询机制Kafka不支持消息查询RocketMQ支持根据Message Id查询消息，也支持根据消息内容查询消息（发送消息时指定一个Message Key，任意字符串，例如指定为订单Id） 消息回溯Kafka理论上可以按照Offset来回溯消息RocketMQ支持按照时间来回溯消息，精度毫秒，例如从一天之前的某时某分某秒开始重新消费消息……… Copyright © 2020-2021 ChangDingFang all right reserved，powered by Gitbook最近一次修订时间: 2021-08-25 18:31:45 "}}